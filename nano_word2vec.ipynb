{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211c9c16-4cb3-4586-9eb4-dfa2ae0d4f00",
   "metadata": {},
   "source": [
    "# nano word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd567f35-4edf-4320-8c8d-f9fb6b060594",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bdaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = 'weights.bak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82073b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 8\n",
    "n_embd = 96\n",
    "n_hidden = 96\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135090dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/generics_kb\n",
    "\n",
    "datasets = load_dataset(\"generics_kb\", \"generics_kb_simplewiki\")\n",
    "dataset = datasets[\"train\"]\n",
    "print(f'{len(dataset)=} {dataset[0].keys()=}')\n",
    "\n",
    "\n",
    "charset_whitelist = 'abcdefghijklmnopqrstuvwxyz- '\n",
    "def sanitize(s):\n",
    "    return ''.join([c for c in s.lower() if c in charset_whitelist])\n",
    "\n",
    "sentences = [sanitize(d['sentence']) for d in dataset]\n",
    "print(f'{sentences[:3]=}')\n",
    "print(f'{max([len(s.split()) for s in sentences])=}')\n",
    "\n",
    "vocab = set([w for s in sentences for w in s.split()])\n",
    "print(f'{len(vocab)=} {list(vocab)[:3]=}')\n",
    "\n",
    "# The sample size for each word seems really small so this dataset probably won't work at all.\n",
    "# can I get a dataset specialized on fruits maybe, to do queries of the type `lemon - yellow + green = lime`\n",
    "queen = [s for s in sentences if 'queen' in s]\n",
    "print(f'{len(queen)=} {queen[:3]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['<end>', '<???>'] + list(vocab)\n",
    "vocab_size = len(vocab_list)\n",
    "stoi = {w: i for i, w in enumerate(vocab_list)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return torch.tensor([stoi.get(w, 1) for w in sanitize(s).split() + ['<end>']], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    t = t.tolist() if isinstance(t, torch.Tensor) else t\n",
    "    return ' '.join([itos[i] for i in t])\n",
    "\n",
    "# careful here if we use words outside of vocab it'll explode\n",
    "for xs in ['I for one welcome our new robot overlords', 'The chicken cross the road']:\n",
    "    print(f'{encode(xs)=}')\n",
    "    print(f'{decode(encode(xs))=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f98650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape the data for training\n",
    "def chunk(s):\n",
    "    s = torch.cat((torch.zeros(block_size, dtype=torch.long), s))\n",
    "    for i in range(0, len(s) - block_size):\n",
    "        yield s[i: i + block_size], s[i + 1: i + block_size + 1]\n",
    "\n",
    "chunked = [c for s in sentences for c in chunk(encode(s))]\n",
    "Xtrain = [c[0] for c in chunked]\n",
    "Ytrain = [c[1] for c in chunked]\n",
    "\n",
    "for i in range(3):\n",
    "    print(Xtrain[i], Ytrain[i])\n",
    "    print(f'{decode(Xtrain[i])=} {decode(Ytrain[i])=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    # TODO: swap between train and val\n",
    "    ix = torch.randint(len(Xtrain), (batch_size,))\n",
    "    x = torch.stack([Xtrain[i] for i in ix])\n",
    "    y = torch.stack([Ytrain[i] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb[:2])\n",
    "print(yb[:2])\n",
    "print(f'{decode(xb[0])} -> {decode(yb[0])}')\n",
    "print(f'{decode(xb[1])} -> {decode(yb[1])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea70544",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4f331",
   "metadata": {},
   "source": [
    "## Implem the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0xdeadbeef) # for reproducibility\n",
    "\n",
    "class Bnorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # /!\\\n",
    "        # /!\\ it looks insanely expensive, this 10x the training time\n",
    "        # /!\\\n",
    "        return self.bn(x.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.layers = nn.Sequential(\n",
    "            # nn.Linear(n_embd, n_hidden), Bnorm(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_hidden), nn.ReLU(),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # print(f'{idx.shape=} {targets.shape=}')\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        x = self.layers(tok_emb)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # juggle with tensor shapes to match pytorch's cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = LM()\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64677e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        loss = estimate_loss()\n",
    "        print(f'step {iter}: train loss {loss:.4f}')\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa748d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup to disk\n",
    "# torch.save(model.state_dict(), WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "# m2 = LM()\n",
    "# m2.load_state_dict(torch.load(WEIGHT_PATH))\n",
    "# m2 = m2.to(device)\n",
    "# m2.eval()\n",
    "\n",
    "# context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "# print(decode(m2.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba702cd",
   "metadata": {},
   "source": [
    "## Can we do anything with embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def euclidean_dist(a, b):\n",
    "    return torch.sqrt(torch.sum((a - b) ** 2))\n",
    "\n",
    "# Cosine distance\n",
    "def cosine_dist(a, b):\n",
    "    return 1 - (a @ b.T) / (torch.sqrt(torch.sum(a**2)) * torch.sqrt(torch.sum(b**2)))\n",
    "    # return 1 - torch.nn.functional.cosine_similarity(a, b)\n",
    "\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)\n",
    "\n",
    "assert euclidean_dist(a, a) == 0, 'identity'\n",
    "\n",
    "assert cosine_dist(a, a) == 0, 'identity'\n",
    "assert cosine_dist(a, b) == cosine_dist(b, a), 'commutativity'\n",
    "assert cosine_dist(a, b) == 1 - torch.nn.functional.cosine_similarity(a, b, dim=0), 'check formula'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    return m.token_embedding_table(torch.tensor(stoi[word], dtype=torch.long, device=device))\n",
    "\n",
    "king = get_embedding('king')\n",
    "queen = get_embedding('queen')\n",
    "\n",
    "print(f'{euclidean_dist(king, queen)=}')\n",
    "print(f'{cosine_dist(king, queen)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems pretty random to me, cabbage is closer to queen than king is closer to queen :/\n",
    "cabbage = get_embedding('cabbage')\n",
    "print(f'{euclidean_dist(cabbage, queen)=}')\n",
    "print(f'{cosine_dist(cabbage, queen)=}')\n",
    "\n",
    "shadows = get_embedding('shadows')\n",
    "print(f'{euclidean_dist(shadows, queen)=}')\n",
    "print(f'{cosine_dist(shadows, queen)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all embeddings\n",
    "embds = torch.stack([get_embedding(w) for w in vocab_list])\n",
    "embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all pairwise distances\n",
    "def pairwise_euclidean_distance(embds):\n",
    "    xx = torch.sum(embds**2, dim=1)\n",
    "    xy = embds @ embds.T\n",
    "    x2 = xx.view(-1, 1)\n",
    "    return x2 - 2 * xy + xx\n",
    "\n",
    "pwed = pairwise_euclidean_distance(embds)  \n",
    "print(pwed.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4719abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the closest word for each other word in the vocab:\n",
    "#\n",
    "# This is totally non-sensical :(\n",
    "# I don't see any pattern in the results \n",
    "# hyp 1: the dataset is too sparse, only a few mention of each words\n",
    "# hyp 2: the model is too small, it's not able to learn anything\n",
    "# hyp 3: Unknown-unknown, I messed up something\n",
    "e = pwed\n",
    "mask = (torch.ones_like(e) * float('inf')).tril()\n",
    "vals, ind = torch.min((e + mask), dim=1)\n",
    "\n",
    "for i, j in enumerate(ind[:-1]):\n",
    "    print(f'{itos[i]} {itos[j.item()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0270702",
   "metadata": {},
   "source": [
    "# Try 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de17ed3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caea3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dce284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "threshold = 10\n",
    "context_size = 2 # 2 words on each side\n",
    "n_embd = 96\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f806ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text)=100000000\n",
      "len(text.split())=17005207\n",
      "alphabet = \" abcdefghijklmnopqrstuvwxyz\"\n",
      "vocab[:10]=['noho', 'bunia', 'cupped', 'elemond', 'elles', 'biometricians', 'shadowings', 'teodoro', 'sidor', 'ludwich']\n",
      "len(vocab)=253854\n"
     ]
    }
   ],
   "source": [
    "# read another dataset\n",
    "# http://mattmahoney.net/dc/textdata.html\n",
    "\n",
    "with open('text8', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'{len(text)=}')\n",
    "print(f'{len(text.split())=}')\n",
    "print(f'alphabet = \"{\"\".join(sorted(set(text)))}\"')\n",
    "\n",
    "vocab = list(set(text.split()))\n",
    "print(f'{vocab[:10]=}')\n",
    "print(f'{len(vocab)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abcdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted(vocab)[:100]=['a', 'aa', 'aaa', 'aaaa', 'aaaaaacceglllnorst', 'aaaaaaccegllnorrst', 'aaaaaah', 'aaaaaalmrsstt', 'aaaaaannrstyy', 'aaaaabbcdrr', 'aaaaargh', 'aaaargh', 'aaaassembly', 'aaab', 'aaabbbccc', 'aaahh', 'aaai', 'aaake', 'aaan', 'aaargh', 'aaas', 'aaate', 'aab', 'aababb', 'aabach', 'aabba', 'aabbcc', 'aabbirem', 'aabebwuvev', 'aabehlpt', 'aabmup', 'aabre', 'aabybro', 'aac', 'aaca', 'aacca', 'aaccording', 'aachen', 'aachener', 'aachtopf', 'aaci', 'aacis', 'aacisuan', 'aacplus', 'aacr', 'aacs', 'aacvd', 'aad', 'aadgad', 'aadl', 'aadlik', 'aadnani', 'aadvantage', 'aadyam', 'aaemu', 'aaf', 'aafc', 'aafjes', 'aafk', 'aafp', 'aag', 'aagaard', 'aagama', 'aagard', 'aage', 'aagesen', 'aagsin', 'aah', 'aahaaram', 'aahc', 'aahe', 'aahl', 'aahz', 'aai', 'aaib', 'aaiieee', 'aaimmah', 'aairpass', 'aaiun', 'aaiyangar', 'aaj', 'aajker', 'aak', 'aakirkeby', 'aakjaer', 'aakkram', 'aal', 'aalberg', 'aalborg', 'aalborghus', 'aalborgt', 'aalcc', 'aale', 'aalen', 'aalens', 'aalesund', 'aalesunds', 'aaliyah', 'aals', 'aalst']\n",
      "cs.most_common(100)=[('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644), ('is', 183153), ('as', 131815), ('eight', 125285), ('for', 118445), ('s', 116710), ('five', 115789), ('three', 114775), ('was', 112807), ('by', 111831), ('that', 109510), ('four', 108182), ('six', 102145), ('seven', 99683), ('with', 95603), ('on', 91250), ('are', 76527), ('it', 73334), ('from', 72871), ('or', 68945), ('his', 62603), ('an', 61925), ('be', 61281), ('this', 58832), ('which', 54788), ('at', 54576), ('he', 53573), ('also', 44358), ('not', 44033), ('have', 39712), ('were', 39086), ('has', 37866), ('but', 35358), ('other', 32433), ('their', 31523), ('its', 29567), ('first', 28810), ('they', 28553), ('some', 28161), ('had', 28100), ('all', 26229), ('more', 26223), ('most', 25563), ('can', 25519), ('been', 25383), ('such', 24413), ('many', 24096), ('who', 23997), ('new', 23770), ('used', 22737), ('there', 22707), ('after', 21125), ('when', 20623), ('into', 20484), ('american', 20477), ('time', 20412), ('these', 19864), ('only', 19463), ('see', 19206), ('may', 19115), ('than', 18807), ('world', 17949), ('i', 17581), ('b', 17516), ('would', 17377), ('d', 17236), ('no', 16155), ('however', 15861), ('between', 15737), ('about', 15574), ('over', 15122), ('years', 14935), ('states', 14916), ('people', 14696), ('war', 14629), ('during', 14578), ('united', 14494), ('known', 14437), ('if', 14420), ('called', 14151), ('use', 14011), ('th', 13380), ('system', 13296), ('often', 12987), ('state', 12904), ('so', 12722), ('history', 12623), ('will', 12560), ('up', 12445), ('while', 12363), ('where', 12347)]\n",
      "cs[\"aaaaaacceglllnorst\"]=1\n",
      "[(1, 118519), (2, 35297), (3, 17742), (4, 11006), (5, 7649), (6, 5529), (7, 4391), (8, 3626), (9, 2961), (10, 2523)]\n",
      "cs[\"king\"]=7456 cs[\"queen\"]=1940\n"
     ]
    }
   ],
   "source": [
    "# how crappy is my dataset ? :(\n",
    "from collections import Counter\n",
    "\n",
    "# looking at the sorted vocab give me very low confidence in the dataset quality\n",
    "print(f'{sorted(vocab)[:100]=}')\n",
    "cs = Counter(text.split())\n",
    "print(f'{cs.most_common(100)=}')\n",
    "print(f'{cs[\"aaaaaacceglllnorst\"]=}')\n",
    "\n",
    "# lots of words are only mentioned once\n",
    "ccs = Counter(cs.values())\n",
    "print(ccs.most_common(10))\n",
    "\n",
    "# on the plus side 'queen' and 'king' seem well represented\n",
    "print(f'{cs[\"king\"]=} {cs[\"queen\"]=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39592be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(butchered_vocab)=47134\n",
      "len(butchered_text)=16561031\n"
     ]
    }
   ],
   "source": [
    "# let's butcher the dataset ¯\\_(ツ)_/¯\n",
    "# remove all the words that are only mentioned bellow a threshold\n",
    "butchered_vocab = [w for w, c in cs.items() if c >= threshold]\n",
    "butchered_vocab_s = set(butchered_vocab)\n",
    "butchered_text = [w for w in text.split() if w in butchered_vocab_s]\n",
    "\n",
    "print(f'{len(butchered_vocab)=}')\n",
    "print(f'{len(butchered_text)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f3590f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode(xs.split())=tensor([  412,   305,   192, 20460,   785,   439,  2217, 30480])\n",
      "decode(encode(xs.split()))='i for one welcome our new robot overlords'\n",
      "encode(xs.split())=tensor([   15, 15026,  3282,    15,  3098])\n",
      "decode(encode(xs.split()))='the chicken cross the road'\n"
     ]
    }
   ],
   "source": [
    "# encode/decode helpers\n",
    "vocab_size = len(butchered_vocab)\n",
    "stoi = {w: i for i, w in enumerate(butchered_vocab)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "def encode(ws):\n",
    "    return torch.tensor([stoi[w] for w in ws], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    t = t.tolist() if isinstance(t, torch.Tensor) else t\n",
    "    t = [t] if isinstance(t, int) else t\n",
    "    return ' '.join([itos[i] for i in t])\n",
    "\n",
    "for xs in ['i for one welcome our new robot overlords', 'the chicken cross the road']:\n",
    "    print(f'{encode(xs.split())=}')\n",
    "    print(f'{decode(encode(xs.split()))=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "040519f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1]) torch.Size([6, 4])\n",
      "tensor([2]) tensor([0, 1, 3, 4])\n",
      "decode(X[i])='as' decode(Y[i])='anarchism originated a term'\n",
      "tensor([3]) tensor([1, 2, 4, 5])\n",
      "decode(X[i])='a' decode(Y[i])='originated as term of'\n",
      "tensor([4]) tensor([2, 3, 5, 6])\n",
      "decode(X[i])='term' decode(Y[i])='as a of abuse'\n"
     ]
    }
   ],
   "source": [
    "# shape the data for training\n",
    "# using the skip-gram method\n",
    "def chunk(ws):\n",
    "    x, y = [], []\n",
    "    # miss a few words at the beginning and end of the text, w/e\n",
    "    for i in range(context_size, len(ws) - context_size):\n",
    "        x.append(ws[i])\n",
    "        # TODO: here a possible optimization would be to probabilistically discard some of the most common words\n",
    "        # the paper suggest proba to keep the word as:\n",
    "        # $P(w_i) = ({\\sqrt {z(w_i) \\over 0.001} + 1}) . {0.001 \\over z(w_i)}$\n",
    "        # z(w_i) being the frequency of the word in the corpus\n",
    "        y.append(torch.cat((ws[i - context_size: i], ws[i + 1: i + 1 + context_size])))\n",
    "    return torch.tensor(x).view(-1, 1), torch.stack(y)\n",
    "\n",
    "X, Y = chunk(encode(butchered_text[:10]))\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    print(X[i], Y[i])\n",
    "    print(f'{decode(X[i])=} {decode(Y[i])=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19eae3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7],\n",
      "        [2]], device='cuda:0')\n",
      "tensor([[5, 6, 8, 9],\n",
      "        [0, 1, 3, 4]], device='cuda:0')\n",
      "first -> of abuse used against\n",
      "as -> anarchism originated a term\n"
     ]
    }
   ],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(len(X), (batch_size,))\n",
    "    x, y = X[ix], Y[ix]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb[:2])\n",
    "print(yb[:2])\n",
    "print(f'{decode(xb[0])} -> {decode(yb[0])}')\n",
    "print(f'{decode(xb[1])} -> {decode(yb[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd8dba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        x, y = get_batch()\n",
    "        logits, loss = model(x, y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a7774",
   "metadata": {},
   "source": [
    "## Skip-gram model\n",
    "given a word guess the (#context_size) words surrounding it.\n",
    "e.g. \"I for one welcome our robot overlords\"\n",
    "\n",
    "welcome -> for, one, our, robot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
