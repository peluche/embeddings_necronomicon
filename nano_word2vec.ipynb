{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211c9c16-4cb3-4586-9eb4-dfa2ae0d4f00",
   "metadata": {},
   "source": [
    "# nano word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd567f35-4edf-4320-8c8d-f9fb6b060594",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c3cdcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "866bdaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = 'weights.bak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "82073b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 8\n",
    "n_embd = 96\n",
    "n_hidden = 96\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "135090dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset)=12765 dataset[0].keys()=dict_keys(['source_name', 'sentence', 'sentences_before', 'sentences_after', 'concept_name', 'quantifiers', 'id', 'bert_score', 'headings', 'categories'])\n",
      "sentences[:3]=['sepsis happens when the bacterium enters the blood and make it form tiny clots', 'incubation period is only one to two days', 'scuba diving is a common tourist activity']\n",
      "max([len(s.split()) for s in sentences])=22\n",
      "len(vocab)=13477 list(vocab)[:3]=['occasionally', 'technological', 'welding']\n",
      "len(queen)=4 queen[:3]=['monarch is a word that means king or queen', 'pregnant queens deliver their litters by themselves guided by instinct', 'most ant species have a system in which only the queen and breeding females can mate']\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/generics_kb\n",
    "\n",
    "datasets = load_dataset(\"generics_kb\", \"generics_kb_simplewiki\")\n",
    "dataset = datasets[\"train\"]\n",
    "print(f'{len(dataset)=} {dataset[0].keys()=}')\n",
    "\n",
    "\n",
    "charset_whitelist = 'abcdefghijklmnopqrstuvwxyz- '\n",
    "def sanitize(s):\n",
    "    return ''.join([c for c in s.lower() if c in charset_whitelist])\n",
    "\n",
    "sentences = [sanitize(d['sentence']) for d in dataset]\n",
    "print(f'{sentences[:3]=}')\n",
    "print(f'{max([len(s.split()) for s in sentences])=}')\n",
    "\n",
    "vocab = set([w for s in sentences for w in s.split()])\n",
    "print(f'{len(vocab)=} {list(vocab)[:3]=}')\n",
    "\n",
    "# The sample size for each word seems really small so this dataset probably won't work at all.\n",
    "# can I get a dataset specialized on fruits maybe, to do queries of the type `lemon - yellow + green = lime`\n",
    "queen = [s for s in sentences if 'queen' in s]\n",
    "print(f'{len(queen)=} {queen[:3]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e8382d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode(xs)=tensor([    1, 10912,  3840, 12269,  9667,  8109,     1,     1,     0])\n",
      "decode(encode(xs))='<???> for one welcome our new <???> <???> <end>'\n",
      "encode(xs)=tensor([8951, 4067,  614, 8951, 9491,    0])\n",
      "decode(encode(xs))='the chicken cross the road <end>'\n"
     ]
    }
   ],
   "source": [
    "vocab_list = ['<end>', '<???>'] + list(vocab)\n",
    "vocab_size = len(vocab_list)\n",
    "stoi = {w: i for i, w in enumerate(vocab_list)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return torch.tensor([stoi.get(w, 1) for w in sanitize(s).split() + ['<end>']], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    t = t.tolist() if isinstance(t, torch.Tensor) else t\n",
    "    return ' '.join([itos[i] for i in t])\n",
    "\n",
    "# careful here if we use words outside of vocab it'll explode\n",
    "for xs in ['I for one welcome our new robot overlords', 'The chicken cross the road']:\n",
    "    print(f'{encode(xs)=}')\n",
    "    print(f'{decode(encode(xs))=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "73f98650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0]) tensor([   0,    0,    0,    0,    0,    0,    0, 6255])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> <end> <end>' decode(Ytrain[i])='<end> <end> <end> <end> <end> <end> <end> sepsis'\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0, 6255]) tensor([   0,    0,    0,    0,    0,    0, 6255, 8277])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> <end> sepsis' decode(Ytrain[i])='<end> <end> <end> <end> <end> <end> sepsis happens'\n",
      "tensor([   0,    0,    0,    0,    0,    0, 6255, 8277]) tensor([    0,     0,     0,     0,     0,  6255,  8277, 10733])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> sepsis happens' decode(Ytrain[i])='<end> <end> <end> <end> <end> sepsis happens when'\n"
     ]
    }
   ],
   "source": [
    "# shape the data for training\n",
    "def chunk(s):\n",
    "    s = torch.cat((torch.zeros(block_size, dtype=torch.long), s))\n",
    "    for i in range(0, len(s) - block_size):\n",
    "        yield s[i: i + block_size], s[i + 1: i + block_size + 1]\n",
    "\n",
    "chunked = [c for s in sentences for c in chunk(encode(s))]\n",
    "Xtrain = [c[0] for c in chunked]\n",
    "Ytrain = [c[1] for c in chunked]\n",
    "\n",
    "for i in range(3):\n",
    "    print(Xtrain[i], Ytrain[i])\n",
    "    print(f'{decode(Xtrain[i])=} {decode(Ytrain[i])=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "84bd70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,     0,     0, 11415,  1079, 11547],\n",
      "        [    0,     0,     0,     0,     0,  4110,  8469, 10985]],\n",
      "       device='cuda:0')\n",
      "tensor([[    0,     0,     0,     0, 11415,  1079, 11547,   265],\n",
      "        [    0,     0,     0,     0,  4110,  8469, 10985,  2231]],\n",
      "       device='cuda:0')\n",
      "<end> <end> <end> <end> <end> some groups fall -> <end> <end> <end> <end> some groups fall into\n",
      "<end> <end> <end> <end> <end> alleles differ by -> <end> <end> <end> <end> alleles differ by origin\n"
     ]
    }
   ],
   "source": [
    "def get_batch():\n",
    "    # TODO: swap between train and val\n",
    "    ix = torch.randint(len(Xtrain), (batch_size,))\n",
    "    x = torch.stack([Xtrain[i] for i in ix])\n",
    "    y = torch.stack([Ytrain[i] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb[:2])\n",
    "print(yb[:2])\n",
    "print(f'{decode(xb[0])} -> {decode(yb[0])}')\n",
    "print(f'{decode(xb[1])} -> {decode(yb[1])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8ea70544",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4f331",
   "metadata": {},
   "source": [
    "## Implem the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f02b855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 13479])\n",
      "9.51275634765625\n",
      "tensor([ 0.0160,  0.1724,  0.0324,  ..., -0.0516, -0.1353, -0.2061],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0xdeadbeef) # for reproducibility\n",
    "\n",
    "class Bnorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # /!\\\n",
    "        # /!\\ it looks insanely expensive, this 10x the training time\n",
    "        # /!\\\n",
    "        return self.bn(x.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.layers = nn.Sequential(\n",
    "            # nn.Linear(n_embd, n_hidden), Bnorm(n_hidden), nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_hidden), nn.ReLU(),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # print(f'{idx.shape=} {targets.shape=}')\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        x = self.layers(tok_emb)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # juggle with tensor shapes to match pytorch's cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = LM()\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "64677e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c98f7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.5231\n",
      "step 500: train loss 5.3635\n",
      "step 1000: train loss 4.8136\n",
      "step 1500: train loss 4.5534\n",
      "step 2000: train loss 4.4096\n",
      "step 2500: train loss 4.3419\n",
      "step 3000: train loss 4.2829\n",
      "step 3500: train loss 4.2481\n",
      "step 4000: train loss 4.1590\n",
      "step 4500: train loss 4.1624\n",
      "step 5000: train loss 4.0800\n",
      "step 5500: train loss 4.0351\n",
      "step 6000: train loss 4.0066\n",
      "step 6500: train loss 3.9771\n",
      "step 7000: train loss 3.9315\n",
      "step 7500: train loss 3.8949\n",
      "step 8000: train loss 3.8599\n",
      "step 8500: train loss 3.8088\n",
      "step 9000: train loss 3.8183\n",
      "step 9500: train loss 3.8041\n",
      "step 10000: train loss 3.7543\n",
      "step 10500: train loss 3.7266\n",
      "step 11000: train loss 3.7032\n",
      "step 11500: train loss 3.6911\n",
      "step 12000: train loss 3.6786\n",
      "step 12500: train loss 3.6323\n",
      "step 13000: train loss 3.6286\n",
      "step 13500: train loss 3.6087\n",
      "step 14000: train loss 3.6118\n",
      "step 14500: train loss 3.5803\n",
      "step 15000: train loss 3.5565\n",
      "step 15500: train loss 3.5401\n",
      "step 16000: train loss 3.5322\n",
      "step 16500: train loss 3.5141\n",
      "step 17000: train loss 3.4894\n",
      "step 17500: train loss 3.4936\n",
      "step 18000: train loss 3.4616\n",
      "step 18500: train loss 3.4558\n",
      "step 19000: train loss 3.4522\n",
      "step 19500: train loss 3.4466\n",
      "step 20000: train loss 3.4518\n",
      "step 20500: train loss 3.4236\n",
      "step 21000: train loss 3.4378\n",
      "step 21500: train loss 3.3743\n",
      "step 22000: train loss 3.3537\n",
      "step 22500: train loss 3.4156\n",
      "step 23000: train loss 3.3390\n",
      "step 23500: train loss 3.3340\n",
      "step 24000: train loss 3.3256\n",
      "step 24500: train loss 3.3168\n",
      "step 25000: train loss 3.3181\n",
      "step 25500: train loss 3.3015\n",
      "step 26000: train loss 3.3286\n",
      "step 26500: train loss 3.3385\n",
      "step 27000: train loss 3.3116\n",
      "step 27500: train loss 3.3136\n",
      "step 28000: train loss 3.2716\n",
      "step 28500: train loss 3.2705\n",
      "step 29000: train loss 3.2594\n",
      "step 29500: train loss 3.2424\n",
      "step 30000: train loss 3.2568\n",
      "step 30500: train loss 3.2285\n",
      "step 31000: train loss 3.2259\n",
      "step 31500: train loss 3.2534\n",
      "step 32000: train loss 3.2503\n",
      "step 32500: train loss 3.2092\n",
      "step 33000: train loss 3.2335\n",
      "step 33500: train loss 3.2306\n",
      "step 34000: train loss 3.2010\n",
      "step 34500: train loss 3.1914\n",
      "step 35000: train loss 3.2053\n",
      "step 35500: train loss 3.2247\n",
      "step 36000: train loss 3.2011\n",
      "step 36500: train loss 3.1884\n",
      "step 37000: train loss 3.1732\n",
      "step 37500: train loss 3.1773\n",
      "step 38000: train loss 3.1962\n",
      "step 38500: train loss 3.1610\n",
      "step 39000: train loss 3.1484\n",
      "step 39500: train loss 3.1609\n",
      "step 40000: train loss 3.1544\n",
      "step 40500: train loss 3.1738\n",
      "step 41000: train loss 3.1376\n",
      "step 41500: train loss 3.1153\n",
      "step 42000: train loss 3.1134\n",
      "step 42500: train loss 3.1307\n",
      "step 43000: train loss 3.1192\n",
      "step 43500: train loss 3.1198\n",
      "step 44000: train loss 3.1173\n",
      "step 44500: train loss 3.1237\n",
      "step 45000: train loss 3.0978\n",
      "step 45500: train loss 3.1067\n",
      "step 46000: train loss 3.0941\n",
      "step 46500: train loss 3.0907\n",
      "step 47000: train loss 3.1012\n",
      "step 47500: train loss 3.1089\n",
      "step 48000: train loss 3.1040\n",
      "step 48500: train loss 3.0972\n",
      "step 49000: train loss 3.0634\n",
      "step 49500: train loss 3.0635\n",
      "step 50000: train loss 3.0809\n",
      "step 50500: train loss 3.0671\n",
      "step 51000: train loss 3.0673\n",
      "step 51500: train loss 3.0798\n",
      "step 52000: train loss 3.0825\n",
      "step 52500: train loss 3.0715\n",
      "step 53000: train loss 3.0647\n",
      "step 53500: train loss 3.0746\n",
      "step 54000: train loss 3.0423\n",
      "step 54500: train loss 3.0443\n",
      "step 55000: train loss 3.0196\n",
      "step 55500: train loss 3.0303\n",
      "step 56000: train loss 3.0427\n",
      "step 56500: train loss 3.0564\n",
      "step 57000: train loss 3.0524\n",
      "step 57500: train loss 3.0158\n",
      "step 58000: train loss 3.0282\n",
      "step 58500: train loss 3.0216\n",
      "step 59000: train loss 3.0172\n",
      "step 59500: train loss 3.0093\n",
      "step 60000: train loss 3.0222\n",
      "step 60500: train loss 3.0048\n",
      "step 61000: train loss 3.0157\n",
      "step 61500: train loss 2.9923\n",
      "step 62000: train loss 2.9998\n",
      "step 62500: train loss 3.0222\n",
      "step 63000: train loss 2.9990\n",
      "step 63500: train loss 3.0195\n",
      "step 64000: train loss 3.0063\n",
      "step 64500: train loss 2.9896\n",
      "step 65000: train loss 2.9785\n",
      "step 65500: train loss 2.9941\n",
      "step 66000: train loss 2.9891\n",
      "step 66500: train loss 2.9801\n",
      "step 67000: train loss 3.0039\n",
      "step 67500: train loss 2.9973\n",
      "step 68000: train loss 2.9675\n",
      "step 68500: train loss 2.9609\n",
      "step 69000: train loss 2.9710\n",
      "step 69500: train loss 2.9673\n",
      "step 70000: train loss 2.9668\n",
      "step 70500: train loss 2.9650\n",
      "step 71000: train loss 2.9638\n",
      "step 71500: train loss 2.9799\n",
      "step 72000: train loss 2.9489\n",
      "step 72500: train loss 2.9520\n",
      "step 73000: train loss 2.9590\n",
      "step 73500: train loss 2.9655\n",
      "step 74000: train loss 2.9598\n",
      "step 74500: train loss 2.9457\n",
      "step 75000: train loss 2.9598\n",
      "step 75500: train loss 2.9793\n",
      "step 76000: train loss 2.9466\n",
      "step 76500: train loss 2.9594\n",
      "step 77000: train loss 2.9336\n",
      "step 77500: train loss 2.9473\n",
      "step 78000: train loss 2.9495\n",
      "step 78500: train loss 2.9397\n",
      "step 79000: train loss 2.9467\n",
      "step 79500: train loss 2.9357\n",
      "step 80000: train loss 2.9269\n",
      "step 80500: train loss 2.9371\n",
      "step 81000: train loss 2.9564\n",
      "step 81500: train loss 2.9354\n",
      "step 82000: train loss 2.9533\n",
      "step 82500: train loss 2.8998\n",
      "step 83000: train loss 2.9473\n",
      "step 83500: train loss 2.9224\n",
      "step 84000: train loss 2.9422\n",
      "step 84500: train loss 2.9258\n",
      "step 85000: train loss 2.9391\n",
      "step 85500: train loss 2.9132\n",
      "step 86000: train loss 2.9147\n",
      "step 86500: train loss 2.9154\n",
      "step 87000: train loss 2.9133\n",
      "step 87500: train loss 2.9103\n",
      "step 88000: train loss 2.9263\n",
      "step 88500: train loss 2.8826\n",
      "step 89000: train loss 2.9230\n",
      "step 89500: train loss 2.9273\n",
      "step 90000: train loss 2.9123\n",
      "step 90500: train loss 2.9235\n",
      "step 91000: train loss 2.9100\n",
      "step 91500: train loss 2.9271\n",
      "step 92000: train loss 2.9014\n",
      "step 92500: train loss 2.9013\n",
      "step 93000: train loss 2.9149\n",
      "step 93500: train loss 2.8922\n",
      "step 94000: train loss 2.8887\n",
      "step 94500: train loss 2.9150\n",
      "step 95000: train loss 2.8904\n",
      "step 95500: train loss 2.9149\n",
      "step 96000: train loss 2.8801\n",
      "step 96500: train loss 2.9231\n",
      "step 97000: train loss 2.8925\n",
      "step 97500: train loss 2.9133\n",
      "step 98000: train loss 2.8958\n",
      "step 98500: train loss 2.8929\n",
      "step 99000: train loss 2.9015\n",
      "step 99500: train loss 2.8858\n",
      "step 100000: train loss 2.8723\n",
      "step 100500: train loss 2.8661\n",
      "step 101000: train loss 2.9036\n",
      "step 101500: train loss 2.8888\n",
      "step 102000: train loss 2.8709\n",
      "step 102500: train loss 2.9054\n",
      "step 103000: train loss 2.8965\n",
      "step 103500: train loss 2.8685\n",
      "step 104000: train loss 2.8769\n",
      "step 104500: train loss 2.8937\n",
      "step 105000: train loss 2.8823\n",
      "step 105500: train loss 2.8735\n",
      "step 106000: train loss 2.8842\n",
      "step 106500: train loss 2.8867\n",
      "step 107000: train loss 2.8771\n",
      "step 107500: train loss 2.8864\n",
      "step 108000: train loss 2.8724\n",
      "step 108500: train loss 2.8843\n",
      "step 109000: train loss 2.8939\n",
      "step 109500: train loss 2.8681\n",
      "step 110000: train loss 2.8814\n",
      "step 110500: train loss 2.8739\n",
      "step 111000: train loss 2.8720\n",
      "step 111500: train loss 2.8872\n",
      "step 112000: train loss 2.8628\n",
      "step 112500: train loss 2.8921\n",
      "step 113000: train loss 2.8859\n",
      "step 113500: train loss 2.8716\n",
      "step 114000: train loss 2.8543\n",
      "step 114500: train loss 2.8778\n",
      "step 115000: train loss 2.8494\n",
      "step 115500: train loss 2.8571\n",
      "step 116000: train loss 2.8578\n",
      "step 116500: train loss 2.8542\n",
      "step 117000: train loss 2.8412\n",
      "step 117500: train loss 2.8582\n",
      "step 118000: train loss 2.8571\n",
      "step 118500: train loss 2.8690\n",
      "step 119000: train loss 2.8573\n",
      "step 119500: train loss 2.8468\n",
      "step 120000: train loss 2.8468\n",
      "step 120500: train loss 2.8507\n",
      "step 121000: train loss 2.8578\n",
      "step 121500: train loss 2.8692\n",
      "step 122000: train loss 2.8374\n",
      "step 122500: train loss 2.8455\n",
      "step 123000: train loss 2.8611\n",
      "step 123500: train loss 2.8469\n",
      "step 124000: train loss 2.8644\n",
      "step 124500: train loss 2.8553\n",
      "step 125000: train loss 2.8672\n",
      "step 125500: train loss 2.8358\n",
      "step 126000: train loss 2.8745\n",
      "step 126500: train loss 2.8798\n",
      "step 127000: train loss 2.8418\n",
      "step 127500: train loss 2.8566\n",
      "step 128000: train loss 2.8685\n",
      "step 128500: train loss 2.8604\n",
      "step 129000: train loss 2.8670\n",
      "step 129500: train loss 2.8366\n",
      "step 130000: train loss 2.8289\n",
      "step 130500: train loss 2.8669\n",
      "step 131000: train loss 2.8519\n",
      "step 131500: train loss 2.8424\n",
      "step 132000: train loss 2.8362\n",
      "step 132500: train loss 2.8552\n",
      "step 133000: train loss 2.8452\n",
      "step 133500: train loss 2.8539\n",
      "step 134000: train loss 2.8727\n",
      "step 134500: train loss 2.8713\n",
      "step 135000: train loss 2.8565\n",
      "step 135500: train loss 2.8512\n",
      "step 136000: train loss 2.8324\n",
      "step 136500: train loss 2.8366\n",
      "step 137000: train loss 2.8385\n",
      "step 137500: train loss 2.8224\n",
      "step 138000: train loss 2.8472\n",
      "step 138500: train loss 2.8585\n",
      "step 139000: train loss 2.8164\n",
      "step 139500: train loss 2.8240\n",
      "step 140000: train loss 2.8492\n",
      "step 140500: train loss 2.8308\n",
      "step 141000: train loss 2.8436\n",
      "step 141500: train loss 2.8395\n",
      "step 142000: train loss 2.8275\n",
      "step 142500: train loss 2.8307\n",
      "step 143000: train loss 2.8501\n",
      "step 143500: train loss 2.8348\n",
      "step 144000: train loss 2.8480\n",
      "step 144500: train loss 2.8379\n",
      "step 145000: train loss 2.8204\n",
      "step 145500: train loss 2.8437\n",
      "step 146000: train loss 2.8382\n",
      "step 146500: train loss 2.8302\n",
      "step 147000: train loss 2.8412\n",
      "step 147500: train loss 2.8445\n",
      "step 148000: train loss 2.8362\n",
      "step 148500: train loss 2.8475\n",
      "step 149000: train loss 2.8289\n",
      "step 149500: train loss 2.8298\n",
      "step 150000: train loss 2.8135\n",
      "step 150500: train loss 2.8388\n",
      "step 151000: train loss 2.8368\n",
      "step 151500: train loss 2.8428\n",
      "step 152000: train loss 2.8204\n",
      "step 152500: train loss 2.8324\n",
      "step 153000: train loss 2.8371\n",
      "step 153500: train loss 2.8179\n",
      "step 154000: train loss 2.8382\n",
      "step 154500: train loss 2.8305\n",
      "step 155000: train loss 2.8341\n",
      "step 155500: train loss 2.8311\n",
      "step 156000: train loss 2.8236\n",
      "step 156500: train loss 2.8452\n",
      "step 157000: train loss 2.7981\n",
      "step 157500: train loss 2.8299\n",
      "step 158000: train loss 2.8287\n",
      "step 158500: train loss 2.8606\n",
      "step 159000: train loss 2.8261\n",
      "step 159500: train loss 2.8282\n",
      "step 160000: train loss 2.8192\n",
      "step 160500: train loss 2.8236\n",
      "step 161000: train loss 2.8340\n",
      "step 161500: train loss 2.8286\n",
      "step 162000: train loss 2.8263\n",
      "step 162500: train loss 2.8344\n",
      "step 163000: train loss 2.8271\n",
      "step 163500: train loss 2.8159\n",
      "step 164000: train loss 2.8452\n",
      "step 164500: train loss 2.8219\n",
      "step 165000: train loss 2.8190\n",
      "step 165500: train loss 2.8029\n",
      "step 166000: train loss 2.8161\n",
      "step 166500: train loss 2.8156\n",
      "step 167000: train loss 2.8159\n",
      "step 167500: train loss 2.8139\n",
      "step 168000: train loss 2.8382\n",
      "step 168500: train loss 2.8256\n",
      "step 169000: train loss 2.8373\n",
      "step 169500: train loss 2.8149\n",
      "step 170000: train loss 2.8298\n",
      "step 170500: train loss 2.8116\n",
      "step 171000: train loss 2.8382\n",
      "step 171500: train loss 2.8110\n",
      "step 172000: train loss 2.8033\n",
      "step 172500: train loss 2.8255\n",
      "step 173000: train loss 2.8362\n",
      "step 173500: train loss 2.8156\n",
      "step 174000: train loss 2.8244\n",
      "step 174500: train loss 2.8170\n",
      "step 175000: train loss 2.7955\n",
      "step 175500: train loss 2.8225\n",
      "step 176000: train loss 2.8365\n",
      "step 176500: train loss 2.8071\n",
      "step 177000: train loss 2.8164\n",
      "step 177500: train loss 2.8120\n",
      "step 178000: train loss 2.8280\n",
      "step 178500: train loss 2.8104\n",
      "step 179000: train loss 2.8296\n",
      "step 179500: train loss 2.8247\n",
      "step 180000: train loss 2.8266\n",
      "step 180500: train loss 2.8127\n",
      "step 181000: train loss 2.8230\n",
      "step 181500: train loss 2.8136\n",
      "step 182000: train loss 2.7910\n",
      "step 182500: train loss 2.8127\n",
      "step 183000: train loss 2.8161\n",
      "step 183500: train loss 2.8296\n",
      "step 184000: train loss 2.8089\n",
      "step 184500: train loss 2.8282\n",
      "step 185000: train loss 2.8034\n",
      "step 185500: train loss 2.8217\n",
      "step 186000: train loss 2.8310\n",
      "step 186500: train loss 2.7958\n",
      "step 187000: train loss 2.7893\n",
      "step 187500: train loss 2.7905\n",
      "step 188000: train loss 2.8180\n",
      "step 188500: train loss 2.8057\n",
      "step 189000: train loss 2.8121\n",
      "step 189500: train loss 2.8144\n",
      "step 190000: train loss 2.8209\n",
      "step 190500: train loss 2.8238\n",
      "step 191000: train loss 2.8024\n",
      "step 191500: train loss 2.8095\n",
      "step 192000: train loss 2.8109\n",
      "step 192500: train loss 2.7988\n",
      "step 193000: train loss 2.8200\n",
      "step 193500: train loss 2.8203\n",
      "step 194000: train loss 2.8133\n",
      "step 194500: train loss 2.7978\n",
      "step 195000: train loss 2.8005\n",
      "step 195500: train loss 2.8096\n",
      "step 196000: train loss 2.8077\n",
      "step 196500: train loss 2.7998\n",
      "step 197000: train loss 2.8012\n",
      "step 197500: train loss 2.7930\n",
      "step 198000: train loss 2.8306\n",
      "step 198500: train loss 2.8048\n",
      "step 199000: train loss 2.8352\n",
      "step 199500: train loss 2.7911\n",
      "step 200000: train loss 2.8111\n",
      "step 200500: train loss 2.7979\n",
      "step 201000: train loss 2.8077\n",
      "step 201500: train loss 2.8002\n",
      "step 202000: train loss 2.8196\n",
      "step 202500: train loss 2.8007\n",
      "step 203000: train loss 2.8223\n",
      "step 203500: train loss 2.8263\n",
      "step 204000: train loss 2.8176\n",
      "step 204500: train loss 2.8290\n",
      "step 205000: train loss 2.8120\n",
      "step 205500: train loss 2.7956\n",
      "step 206000: train loss 2.7910\n",
      "step 206500: train loss 2.8022\n",
      "step 207000: train loss 2.7973\n",
      "step 207500: train loss 2.7960\n",
      "step 208000: train loss 2.7971\n",
      "step 208500: train loss 2.8261\n",
      "step 209000: train loss 2.8144\n",
      "step 209500: train loss 2.7904\n",
      "step 210000: train loss 2.8113\n",
      "step 210500: train loss 2.8260\n",
      "step 211000: train loss 2.7963\n",
      "step 211500: train loss 2.8147\n",
      "step 212000: train loss 2.7996\n",
      "step 212500: train loss 2.7932\n",
      "step 213000: train loss 2.7874\n",
      "step 213500: train loss 2.7999\n",
      "step 214000: train loss 2.7979\n",
      "step 214500: train loss 2.8212\n",
      "step 215000: train loss 2.8038\n",
      "step 215500: train loss 2.8016\n",
      "step 216000: train loss 2.8200\n",
      "step 216500: train loss 2.7998\n",
      "step 217000: train loss 2.8240\n",
      "step 217500: train loss 2.8311\n",
      "step 218000: train loss 2.8093\n",
      "step 218500: train loss 2.8063\n",
      "step 219000: train loss 2.7862\n",
      "step 219500: train loss 2.7654\n",
      "step 220000: train loss 2.8014\n",
      "step 220500: train loss 2.7874\n",
      "step 221000: train loss 2.7975\n",
      "step 221500: train loss 2.8171\n",
      "step 222000: train loss 2.8040\n",
      "step 222500: train loss 2.8242\n",
      "step 223000: train loss 2.8052\n",
      "step 223500: train loss 2.8171\n",
      "step 224000: train loss 2.8096\n",
      "step 224500: train loss 2.8054\n",
      "step 225000: train loss 2.8062\n",
      "step 225500: train loss 2.7929\n",
      "step 226000: train loss 2.8048\n",
      "step 226500: train loss 2.7911\n",
      "step 227000: train loss 2.7996\n",
      "step 227500: train loss 2.8209\n",
      "step 228000: train loss 2.7955\n",
      "step 228500: train loss 2.7916\n",
      "step 229000: train loss 2.8132\n",
      "step 229500: train loss 2.7897\n",
      "step 230000: train loss 2.8296\n",
      "step 230500: train loss 2.8351\n",
      "step 231000: train loss 2.8130\n",
      "step 231500: train loss 2.7893\n",
      "step 232000: train loss 2.8157\n",
      "step 232500: train loss 2.8032\n",
      "step 233000: train loss 2.7927\n",
      "step 233500: train loss 2.8113\n",
      "step 234000: train loss 2.7904\n",
      "step 234500: train loss 2.8059\n",
      "step 235000: train loss 2.7825\n",
      "step 235500: train loss 2.7973\n",
      "step 236000: train loss 2.7647\n",
      "step 236500: train loss 2.8023\n",
      "step 237000: train loss 2.7897\n",
      "step 237500: train loss 2.8006\n",
      "step 238000: train loss 2.8096\n",
      "step 238500: train loss 2.7982\n",
      "step 239000: train loss 2.8000\n",
      "step 239500: train loss 2.8076\n",
      "step 240000: train loss 2.8166\n",
      "step 240500: train loss 2.8113\n",
      "step 241000: train loss 2.8016\n",
      "step 241500: train loss 2.7751\n",
      "step 242000: train loss 2.7785\n",
      "step 242500: train loss 2.8189\n",
      "step 243000: train loss 2.7992\n",
      "step 243500: train loss 2.7786\n",
      "step 244000: train loss 2.8142\n",
      "step 244500: train loss 2.7986\n",
      "step 245000: train loss 2.7990\n",
      "step 245500: train loss 2.8034\n",
      "step 246000: train loss 2.7957\n",
      "step 246500: train loss 2.8081\n",
      "step 247000: train loss 2.8074\n",
      "step 247500: train loss 2.7898\n",
      "step 248000: train loss 2.7813\n",
      "step 248500: train loss 2.8039\n",
      "step 249000: train loss 2.7905\n",
      "step 249500: train loss 2.7839\n",
      "step 250000: train loss 2.8147\n",
      "step 250500: train loss 2.8215\n",
      "step 251000: train loss 2.8047\n",
      "step 251500: train loss 2.8113\n",
      "step 252000: train loss 2.8035\n",
      "step 252500: train loss 2.8015\n",
      "step 253000: train loss 2.8072\n",
      "step 253500: train loss 2.7888\n",
      "step 254000: train loss 2.8037\n",
      "step 254500: train loss 2.7776\n",
      "step 255000: train loss 2.8004\n",
      "step 255500: train loss 2.8004\n",
      "step 256000: train loss 2.7911\n",
      "step 256500: train loss 2.8013\n",
      "step 257000: train loss 2.8228\n",
      "step 257500: train loss 2.8088\n",
      "step 258000: train loss 2.8071\n",
      "step 258500: train loss 2.8027\n",
      "step 259000: train loss 2.8175\n",
      "step 259500: train loss 2.7797\n",
      "step 260000: train loss 2.7721\n",
      "step 260500: train loss 2.7992\n",
      "step 261000: train loss 2.7867\n",
      "step 261500: train loss 2.7969\n",
      "step 262000: train loss 2.8290\n",
      "step 262500: train loss 2.7879\n",
      "step 263000: train loss 2.7975\n",
      "step 263500: train loss 2.7820\n",
      "step 264000: train loss 2.7865\n",
      "step 264500: train loss 2.7805\n",
      "step 265000: train loss 2.7690\n",
      "step 265500: train loss 2.7936\n",
      "step 266000: train loss 2.8072\n",
      "step 266500: train loss 2.7977\n",
      "step 267000: train loss 2.7865\n",
      "step 267500: train loss 2.8097\n",
      "step 268000: train loss 2.8164\n",
      "step 268500: train loss 2.7910\n",
      "step 269000: train loss 2.7972\n",
      "step 269500: train loss 2.8016\n",
      "step 270000: train loss 2.8021\n",
      "step 270500: train loss 2.7914\n",
      "step 271000: train loss 2.7933\n",
      "step 271500: train loss 2.7887\n",
      "step 272000: train loss 2.7953\n",
      "step 272500: train loss 2.7964\n",
      "step 273000: train loss 2.7933\n",
      "step 273500: train loss 2.8005\n",
      "step 274000: train loss 2.7832\n",
      "step 274500: train loss 2.7928\n",
      "step 275000: train loss 2.7863\n",
      "step 275500: train loss 2.8005\n",
      "step 276000: train loss 2.7863\n",
      "step 276500: train loss 2.7880\n",
      "step 277000: train loss 2.8145\n",
      "step 277500: train loss 2.7967\n",
      "step 278000: train loss 2.7878\n",
      "step 278500: train loss 2.7625\n",
      "step 279000: train loss 2.7906\n",
      "step 279500: train loss 2.8178\n",
      "step 280000: train loss 2.7972\n",
      "step 280500: train loss 2.8209\n",
      "step 281000: train loss 2.7848\n",
      "step 281500: train loss 2.8295\n",
      "step 282000: train loss 2.8078\n",
      "step 282500: train loss 2.7805\n",
      "step 283000: train loss 2.8104\n",
      "step 283500: train loss 2.7877\n",
      "step 284000: train loss 2.7955\n",
      "step 284500: train loss 2.8267\n",
      "step 285000: train loss 2.7681\n",
      "step 285500: train loss 2.7829\n",
      "step 286000: train loss 2.7959\n",
      "step 286500: train loss 2.7980\n",
      "step 287000: train loss 2.7878\n",
      "step 287500: train loss 2.7911\n",
      "step 288000: train loss 2.7862\n",
      "step 288500: train loss 2.7782\n",
      "step 289000: train loss 2.7782\n",
      "step 289500: train loss 2.8092\n",
      "step 290000: train loss 2.7890\n",
      "step 290500: train loss 2.7880\n",
      "step 291000: train loss 2.7922\n",
      "step 291500: train loss 2.8138\n",
      "step 292000: train loss 2.7944\n",
      "step 292500: train loss 2.7840\n",
      "step 293000: train loss 2.7826\n",
      "step 293500: train loss 2.7898\n",
      "step 294000: train loss 2.7938\n",
      "step 294500: train loss 2.7868\n",
      "step 295000: train loss 2.7802\n",
      "step 295500: train loss 2.7759\n",
      "step 296000: train loss 2.8012\n",
      "step 296500: train loss 2.8014\n",
      "step 297000: train loss 2.8026\n",
      "step 297500: train loss 2.7869\n",
      "step 298000: train loss 2.7901\n",
      "step 298500: train loss 2.7989\n",
      "step 299000: train loss 2.7786\n",
      "step 299500: train loss 2.7944\n",
      "step 300000: train loss 2.7887\n",
      "step 300500: train loss 2.7780\n",
      "step 301000: train loss 2.7893\n",
      "step 301500: train loss 2.7989\n",
      "step 302000: train loss 2.8144\n",
      "step 302500: train loss 2.7830\n",
      "step 303000: train loss 2.8112\n",
      "step 303500: train loss 2.8222\n",
      "step 304000: train loss 2.7910\n",
      "step 304500: train loss 2.7902\n",
      "step 305000: train loss 2.8117\n",
      "step 305500: train loss 2.7958\n",
      "step 306000: train loss 2.7922\n",
      "step 306500: train loss 2.8049\n",
      "step 307000: train loss 2.7691\n",
      "step 307500: train loss 2.7939\n",
      "step 308000: train loss 2.7838\n",
      "step 308500: train loss 2.7779\n",
      "step 309000: train loss 2.7976\n",
      "step 309500: train loss 2.7893\n",
      "step 310000: train loss 2.7798\n",
      "step 310500: train loss 2.8120\n",
      "step 311000: train loss 2.7754\n",
      "step 311500: train loss 2.7948\n",
      "step 312000: train loss 2.7954\n",
      "step 312500: train loss 2.7997\n",
      "step 313000: train loss 2.7906\n",
      "step 313500: train loss 2.7643\n",
      "step 314000: train loss 2.7929\n",
      "step 314500: train loss 2.8040\n",
      "step 315000: train loss 2.7939\n",
      "step 315500: train loss 2.7823\n",
      "step 316000: train loss 2.7909\n",
      "step 316500: train loss 2.7943\n",
      "step 317000: train loss 2.7707\n",
      "step 317500: train loss 2.8316\n",
      "step 318000: train loss 2.7789\n",
      "step 318500: train loss 2.8041\n",
      "step 319000: train loss 2.7845\n",
      "step 319500: train loss 2.7958\n",
      "step 320000: train loss 2.7768\n",
      "step 320500: train loss 2.7975\n",
      "step 321000: train loss 2.7951\n",
      "step 321500: train loss 2.7993\n",
      "step 322000: train loss 2.7881\n",
      "step 322500: train loss 2.8117\n",
      "step 323000: train loss 2.7886\n",
      "step 323500: train loss 2.8053\n",
      "step 324000: train loss 2.7967\n",
      "step 324500: train loss 2.7821\n",
      "step 325000: train loss 2.7836\n",
      "step 325500: train loss 2.7926\n",
      "step 326000: train loss 2.7962\n",
      "step 326500: train loss 2.8120\n",
      "step 327000: train loss 2.7733\n",
      "step 327500: train loss 2.7997\n",
      "step 328000: train loss 2.7889\n",
      "step 328500: train loss 2.7924\n",
      "step 329000: train loss 2.7829\n",
      "step 329500: train loss 2.7939\n",
      "step 330000: train loss 2.7879\n",
      "step 330500: train loss 2.7767\n",
      "step 331000: train loss 2.7665\n",
      "step 331500: train loss 2.7764\n",
      "step 332000: train loss 2.7936\n",
      "step 332500: train loss 2.7888\n",
      "step 333000: train loss 2.7844\n",
      "step 333500: train loss 2.7925\n",
      "step 334000: train loss 2.7991\n",
      "step 334500: train loss 2.7868\n",
      "step 335000: train loss 2.7927\n",
      "step 335500: train loss 2.7822\n",
      "step 336000: train loss 2.7946\n",
      "step 336500: train loss 2.7731\n",
      "step 337000: train loss 2.7879\n",
      "step 337500: train loss 2.8024\n",
      "step 338000: train loss 2.7727\n",
      "step 338500: train loss 2.7825\n",
      "step 339000: train loss 2.8012\n",
      "step 339500: train loss 2.7870\n",
      "step 340000: train loss 2.7975\n",
      "step 340500: train loss 2.7734\n",
      "step 341000: train loss 2.7582\n",
      "step 341500: train loss 2.7964\n",
      "step 342000: train loss 2.7885\n",
      "step 342500: train loss 2.7992\n",
      "step 343000: train loss 2.7962\n",
      "step 343500: train loss 2.7970\n",
      "step 344000: train loss 2.7905\n",
      "step 344500: train loss 2.7818\n",
      "step 345000: train loss 2.7549\n",
      "step 345500: train loss 2.8125\n",
      "step 346000: train loss 2.7889\n",
      "step 346500: train loss 2.8030\n",
      "step 347000: train loss 2.7876\n",
      "step 347500: train loss 2.7706\n",
      "step 348000: train loss 2.7790\n",
      "step 348500: train loss 2.7992\n",
      "step 349000: train loss 2.8226\n",
      "step 349500: train loss 2.7745\n",
      "step 350000: train loss 2.8264\n",
      "step 350500: train loss 2.7833\n",
      "step 351000: train loss 2.7988\n",
      "step 351500: train loss 2.7813\n",
      "step 352000: train loss 2.7932\n",
      "step 352500: train loss 2.7737\n",
      "step 353000: train loss 2.7885\n",
      "step 353500: train loss 2.7889\n",
      "step 354000: train loss 2.7852\n",
      "step 354500: train loss 2.8015\n",
      "step 355000: train loss 2.7901\n",
      "step 355500: train loss 2.7946\n",
      "step 356000: train loss 2.8033\n",
      "step 356500: train loss 2.7817\n",
      "step 357000: train loss 2.7764\n",
      "step 357500: train loss 2.7852\n",
      "step 358000: train loss 2.7914\n",
      "step 358500: train loss 2.7803\n",
      "step 359000: train loss 2.8056\n",
      "step 359500: train loss 2.7976\n",
      "step 360000: train loss 2.8073\n",
      "step 360500: train loss 2.8056\n",
      "step 361000: train loss 2.7798\n",
      "step 361500: train loss 2.7801\n",
      "step 362000: train loss 2.8025\n",
      "step 362500: train loss 2.7995\n",
      "step 363000: train loss 2.7859\n",
      "step 363500: train loss 2.7859\n",
      "step 364000: train loss 2.7830\n",
      "step 364500: train loss 2.7990\n",
      "step 365000: train loss 2.7904\n",
      "step 365500: train loss 2.7842\n",
      "step 366000: train loss 2.7873\n",
      "step 366500: train loss 2.7869\n",
      "step 367000: train loss 2.7986\n",
      "step 367500: train loss 2.7795\n",
      "step 368000: train loss 2.8055\n",
      "step 368500: train loss 2.7904\n",
      "step 369000: train loss 2.7936\n",
      "step 369500: train loss 2.7740\n",
      "step 370000: train loss 2.7929\n",
      "step 370500: train loss 2.8034\n",
      "step 371000: train loss 2.7799\n",
      "step 371500: train loss 2.7852\n",
      "step 372000: train loss 2.7859\n",
      "step 372500: train loss 2.7593\n",
      "step 373000: train loss 2.7693\n",
      "step 373500: train loss 2.7948\n",
      "step 374000: train loss 2.8009\n",
      "step 374500: train loss 2.7900\n",
      "step 375000: train loss 2.7722\n",
      "step 375500: train loss 2.7873\n",
      "step 376000: train loss 2.7996\n",
      "step 376500: train loss 2.7883\n",
      "step 377000: train loss 2.7958\n",
      "step 377500: train loss 2.7780\n",
      "step 378000: train loss 2.7831\n",
      "step 378500: train loss 2.7681\n",
      "step 379000: train loss 2.7849\n",
      "step 379500: train loss 2.7620\n",
      "step 380000: train loss 2.7889\n",
      "step 380500: train loss 2.7776\n",
      "step 381000: train loss 2.7993\n",
      "step 381500: train loss 2.7885\n",
      "step 382000: train loss 2.7938\n",
      "step 382500: train loss 2.7985\n",
      "step 383000: train loss 2.7660\n",
      "step 383500: train loss 2.7811\n",
      "step 384000: train loss 2.7705\n",
      "step 384500: train loss 2.8090\n",
      "step 385000: train loss 2.7799\n",
      "step 385500: train loss 2.7778\n",
      "step 386000: train loss 2.7889\n",
      "step 386500: train loss 2.7971\n",
      "step 387000: train loss 2.7886\n",
      "step 387500: train loss 2.7873\n",
      "step 388000: train loss 2.7803\n",
      "step 388500: train loss 2.7867\n",
      "step 389000: train loss 2.7913\n",
      "step 389500: train loss 2.7537\n",
      "step 390000: train loss 2.7686\n",
      "step 390500: train loss 2.7674\n",
      "step 391000: train loss 2.7665\n",
      "step 391500: train loss 2.7970\n",
      "step 392000: train loss 2.7612\n",
      "step 392500: train loss 2.7635\n",
      "step 393000: train loss 2.7783\n",
      "step 393500: train loss 2.7925\n",
      "step 394000: train loss 2.7698\n",
      "step 394500: train loss 2.7666\n",
      "step 395000: train loss 2.7867\n",
      "step 395500: train loss 2.7909\n",
      "step 396000: train loss 2.7971\n",
      "step 396500: train loss 2.7885\n",
      "step 397000: train loss 2.8008\n",
      "step 397500: train loss 2.7933\n",
      "step 398000: train loss 2.7807\n",
      "step 398500: train loss 2.8025\n",
      "step 399000: train loss 2.7586\n",
      "step 399500: train loss 2.7861\n",
      "step 400000: train loss 2.7987\n",
      "step 400500: train loss 2.7880\n",
      "step 401000: train loss 2.7732\n",
      "step 401500: train loss 2.8035\n",
      "step 402000: train loss 2.7861\n",
      "step 402500: train loss 2.7947\n",
      "step 403000: train loss 2.7945\n",
      "step 403500: train loss 2.7809\n",
      "step 404000: train loss 2.8002\n",
      "step 404500: train loss 2.7947\n",
      "step 405000: train loss 2.7806\n",
      "step 405500: train loss 2.7935\n",
      "step 406000: train loss 2.7815\n",
      "step 406500: train loss 2.7704\n",
      "step 407000: train loss 2.7961\n",
      "step 407500: train loss 2.7837\n",
      "step 408000: train loss 2.7734\n",
      "step 408500: train loss 2.7928\n",
      "step 409000: train loss 2.7706\n",
      "step 409500: train loss 2.7930\n",
      "step 410000: train loss 2.8011\n",
      "step 410500: train loss 2.7790\n",
      "step 411000: train loss 2.7827\n",
      "step 411500: train loss 2.7833\n",
      "step 412000: train loss 2.8095\n",
      "step 412500: train loss 2.7796\n",
      "step 413000: train loss 2.7867\n",
      "step 413500: train loss 2.7768\n",
      "step 414000: train loss 2.7657\n",
      "step 414500: train loss 2.7889\n",
      "step 415000: train loss 2.7852\n",
      "step 415500: train loss 2.7618\n",
      "step 416000: train loss 2.7716\n",
      "step 416500: train loss 2.7912\n",
      "step 417000: train loss 2.7696\n",
      "step 417500: train loss 2.7846\n",
      "step 418000: train loss 2.7766\n",
      "step 418500: train loss 2.7817\n",
      "step 419000: train loss 2.7796\n",
      "step 419500: train loss 2.7946\n",
      "step 420000: train loss 2.7841\n",
      "step 420500: train loss 2.7777\n",
      "step 421000: train loss 2.7814\n",
      "step 421500: train loss 2.7695\n",
      "step 422000: train loss 2.7719\n",
      "step 422500: train loss 2.7750\n",
      "step 423000: train loss 2.7690\n",
      "step 423500: train loss 2.7701\n",
      "step 424000: train loss 2.8056\n",
      "step 424500: train loss 2.7969\n",
      "step 425000: train loss 2.7978\n",
      "step 425500: train loss 2.7811\n",
      "step 426000: train loss 2.7972\n",
      "step 426500: train loss 2.7842\n",
      "step 427000: train loss 2.7856\n",
      "step 427500: train loss 2.7784\n",
      "step 428000: train loss 2.7952\n",
      "step 428500: train loss 2.7893\n",
      "step 429000: train loss 2.7699\n",
      "step 429500: train loss 2.7943\n",
      "step 430000: train loss 2.7762\n",
      "step 430500: train loss 2.7730\n",
      "step 431000: train loss 2.7614\n",
      "step 431500: train loss 2.7838\n",
      "step 432000: train loss 2.7811\n",
      "step 432500: train loss 2.7954\n",
      "step 433000: train loss 2.7949\n",
      "step 433500: train loss 2.7822\n",
      "step 434000: train loss 2.7852\n",
      "step 434500: train loss 2.8035\n",
      "step 435000: train loss 2.7928\n",
      "step 435500: train loss 2.7867\n",
      "step 436000: train loss 2.7570\n",
      "step 436500: train loss 2.7956\n",
      "step 437000: train loss 2.7651\n",
      "step 437500: train loss 2.7679\n",
      "step 438000: train loss 2.8060\n",
      "step 438500: train loss 2.7722\n",
      "step 439000: train loss 2.7809\n",
      "step 439500: train loss 2.7923\n",
      "step 440000: train loss 2.7879\n",
      "step 440500: train loss 2.7962\n",
      "step 441000: train loss 2.7995\n",
      "step 441500: train loss 2.7783\n",
      "step 442000: train loss 2.7719\n",
      "step 442500: train loss 2.7625\n",
      "step 443000: train loss 2.7660\n",
      "step 443500: train loss 2.7696\n",
      "step 444000: train loss 2.8105\n",
      "step 444500: train loss 2.7684\n",
      "step 445000: train loss 2.7773\n",
      "step 445500: train loss 2.7946\n",
      "step 446000: train loss 2.7628\n",
      "step 446500: train loss 2.7995\n",
      "step 447000: train loss 2.8035\n",
      "step 447500: train loss 2.7778\n",
      "step 448000: train loss 2.7846\n",
      "step 448500: train loss 2.7812\n",
      "step 449000: train loss 2.7788\n",
      "step 449500: train loss 2.7828\n",
      "step 450000: train loss 2.7959\n",
      "step 450500: train loss 2.7914\n",
      "step 451000: train loss 2.7870\n",
      "step 451500: train loss 2.7822\n",
      "step 452000: train loss 2.7743\n",
      "step 452500: train loss 2.7858\n",
      "step 453000: train loss 2.7885\n",
      "step 453500: train loss 2.7849\n",
      "step 454000: train loss 2.7915\n",
      "step 454500: train loss 2.7996\n",
      "step 455000: train loss 2.7498\n",
      "step 455500: train loss 2.7788\n",
      "step 456000: train loss 2.7783\n",
      "step 456500: train loss 2.7779\n",
      "step 457000: train loss 2.7907\n",
      "step 457500: train loss 2.7845\n",
      "step 458000: train loss 2.7896\n",
      "step 458500: train loss 2.7954\n",
      "step 459000: train loss 2.7774\n",
      "step 459500: train loss 2.8035\n",
      "step 460000: train loss 2.8011\n",
      "step 460500: train loss 2.7811\n",
      "step 461000: train loss 2.7887\n",
      "step 461500: train loss 2.7679\n",
      "step 462000: train loss 2.7652\n",
      "step 462500: train loss 2.7925\n",
      "step 463000: train loss 2.7757\n",
      "step 463500: train loss 2.7958\n",
      "step 464000: train loss 2.8061\n",
      "step 464500: train loss 2.8048\n",
      "step 465000: train loss 2.7709\n",
      "step 465500: train loss 2.7709\n",
      "step 466000: train loss 2.7851\n",
      "step 466500: train loss 2.7794\n",
      "step 467000: train loss 2.7915\n",
      "step 467500: train loss 2.7848\n",
      "step 468000: train loss 2.7669\n",
      "step 468500: train loss 2.7800\n",
      "step 469000: train loss 2.7860\n",
      "step 469500: train loss 2.7946\n",
      "step 470000: train loss 2.7795\n",
      "step 470500: train loss 2.7725\n",
      "step 471000: train loss 2.7960\n",
      "step 471500: train loss 2.7804\n",
      "step 472000: train loss 2.8001\n",
      "step 472500: train loss 2.7747\n",
      "step 473000: train loss 2.7751\n",
      "step 473500: train loss 2.7644\n",
      "step 474000: train loss 2.7753\n",
      "step 474500: train loss 2.7713\n",
      "step 475000: train loss 2.7783\n",
      "step 475500: train loss 2.7852\n",
      "step 476000: train loss 2.8062\n",
      "step 476500: train loss 2.7850\n",
      "step 477000: train loss 2.7714\n",
      "step 477500: train loss 2.8088\n",
      "step 478000: train loss 2.7790\n",
      "step 478500: train loss 2.7813\n",
      "step 479000: train loss 2.7858\n",
      "step 479500: train loss 2.7934\n",
      "step 480000: train loss 2.7850\n",
      "step 480500: train loss 2.7849\n",
      "step 481000: train loss 2.7738\n",
      "step 481500: train loss 2.7749\n",
      "step 482000: train loss 2.7772\n",
      "step 482500: train loss 2.7735\n",
      "step 483000: train loss 2.7764\n",
      "step 483500: train loss 2.7783\n",
      "step 484000: train loss 2.7911\n",
      "step 484500: train loss 2.7800\n",
      "step 485000: train loss 2.7593\n",
      "step 485500: train loss 2.7837\n",
      "step 486000: train loss 2.7756\n",
      "step 486500: train loss 2.8039\n",
      "step 487000: train loss 2.7841\n",
      "step 487500: train loss 2.7987\n",
      "step 488000: train loss 2.7925\n",
      "step 488500: train loss 2.7749\n",
      "step 489000: train loss 2.7733\n",
      "step 489500: train loss 2.7940\n",
      "step 490000: train loss 2.7717\n",
      "step 490500: train loss 2.7715\n",
      "step 491000: train loss 2.7738\n",
      "step 491500: train loss 2.7788\n",
      "step 492000: train loss 2.7833\n",
      "step 492500: train loss 2.7966\n",
      "step 493000: train loss 2.7880\n",
      "step 493500: train loss 2.7954\n",
      "step 494000: train loss 2.7609\n",
      "step 494500: train loss 2.7830\n",
      "step 495000: train loss 2.7835\n",
      "step 495500: train loss 2.7655\n",
      "step 496000: train loss 2.7643\n",
      "step 496500: train loss 2.7768\n",
      "step 497000: train loss 2.7840\n",
      "step 497500: train loss 2.7756\n",
      "step 498000: train loss 2.7711\n",
      "step 498500: train loss 2.7892\n",
      "step 499000: train loss 2.7870\n",
      "step 499500: train loss 2.7815\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        loss = estimate_loss()\n",
    "        print(f'step {iter}: train loss {loss:.4f}')\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2aa748d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<end> <end> <end> <end> <end> <end> <end> <end> <end> movie sets out carbon dioxide or received vibrations of velocity <end> men have sails pushing the border when they lived a crop in place in soil some people can hang far the hazards and length is the most sacred and restore the value to be poisonous liquids <end> <end> <end> <end> <end> <end> <end> winter is the mediterranean sea snakes live in the toes on their own carbohydrates are places on how meanings <end> <end> people have eyes are subunits of static friction is sometimes travel through a like a rigid cell phones use a horizon line or other symptoms are shot and three major important part of a vaccine to make micronations to authors are named after their own special devices to make it easier to passive crossovers are special hardware and the gig is chiles most emergencies as likely than women enlist in saudi arabia is when a large structures before the whole numbers are light can be cheap and african elephants eat small amount of food chain reactions can make the family name of work-up procedure before and most dialects in the cost accounting scholarship is usual fatigue goes to the main activities in the two fertilized eggs so since a thick and task such as powders granules <end> <end> <end> <end> <end> <end> <end> <end> ants look rather than men but bad effect at least two smaller hearing high school between stimuli are some species leave it into elite universities tend to a huge impact on the earths ionosphere they can be found everywhere <end> carbon in very large companies hire paralegals go to speak it was hot in winter sport of good hygiene and anti-clockwise in folk music is illegal immigrants come before things like to a good reaction of the result of any\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3f22aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup to disk\n",
    "torch.save(model.state_dict(), WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "71ef6a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM(\n",
       "  (token_embedding_table): Embedding(13479, 96)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=96, out_features=13479, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from disk\n",
    "m2 = LM()\n",
    "m2.load_state_dict(torch.load(WEIGHT_PATH))\n",
    "m2 = m2.to(device)\n",
    "m2.eval()\n",
    "\n",
    "# context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "# print(decode(m2.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
