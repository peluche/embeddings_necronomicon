{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211c9c16-4cb3-4586-9eb4-dfa2ae0d4f00",
   "metadata": {},
   "source": [
    "# nano word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd567f35-4edf-4320-8c8d-f9fb6b060594",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c3cdcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "82073b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 8\n",
    "n_embd = 64\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "135090dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset)=12765 dataset[0].keys()=dict_keys(['source_name', 'sentence', 'sentences_before', 'sentences_after', 'concept_name', 'quantifiers', 'id', 'bert_score', 'headings', 'categories'])\n",
      "sentences[:3]=['sepsis happens when the bacterium enters the blood and make it form tiny clots', 'incubation period is only one to two days', 'scuba diving is a common tourist activity']\n",
      "max([len(s.split()) for s in sentences])=22\n",
      "len(vocab)=13477 list(vocab)[:3]=['occasionally', 'technological', 'welding']\n",
      "len(queen)=4 queen[:3]=['monarch is a word that means king or queen', 'pregnant queens deliver their litters by themselves guided by instinct', 'most ant species have a system in which only the queen and breeding females can mate']\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/generics_kb\n",
    "\n",
    "datasets = load_dataset(\"generics_kb\", \"generics_kb_simplewiki\")\n",
    "dataset = datasets[\"train\"]\n",
    "print(f'{len(dataset)=} {dataset[0].keys()=}')\n",
    "\n",
    "\n",
    "charset_whitelist = 'abcdefghijklmnopqrstuvwxyz- '\n",
    "def sanitize(s):\n",
    "    return ''.join([c for c in s.lower() if c in charset_whitelist])\n",
    "\n",
    "sentences = [sanitize(d['sentence']) for d in dataset]\n",
    "print(f'{sentences[:3]=}')\n",
    "print(f'{max([len(s.split()) for s in sentences])=}')\n",
    "\n",
    "vocab = set([w for s in sentences for w in s.split()])\n",
    "print(f'{len(vocab)=} {list(vocab)[:3]=}')\n",
    "\n",
    "# The sample size for each word seems really small so this dataset probably won't work at all.\n",
    "# can I get a dataset specialized on fruits maybe, to do queries of the type `lemon - yellow + green = lime`\n",
    "queen = [s for s in sentences if 'queen' in s]\n",
    "print(f'{len(queen)=} {queen[:3]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e8382d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode(xs)=tensor([    1, 10912,  3840, 12269,  9667,  8109,     1,     1,     0])\n",
      "decode(encode(xs))='<???> for one welcome our new <???> <???> <end>'\n",
      "encode(xs)=tensor([8951, 4067,  614, 8951, 9491,    0])\n",
      "decode(encode(xs))='the chicken cross the road <end>'\n"
     ]
    }
   ],
   "source": [
    "vocab_list = ['<end>', '<???>'] + list(vocab)\n",
    "vocab_size = len(vocab_list)\n",
    "stoi = {w: i for i, w in enumerate(vocab_list)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return torch.tensor([stoi.get(w, 1) for w in sanitize(s).split() + ['<end>']], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    return ' '.join([itos[i.item()] for i in t])\n",
    "\n",
    "# careful here if we use words outside of vocab it'll explode\n",
    "for xs in ['I for one welcome our new robot overlords', 'The chicken cross the road']:\n",
    "    print(f'{encode(xs)=}')\n",
    "    print(f'{decode(encode(xs))=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "73f98650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0]) tensor([   0,    0,    0,    0,    0,    0,    0, 6255])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> <end> <end>' decode(Ytrain[i])='<end> <end> <end> <end> <end> <end> <end> sepsis'\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0, 6255]) tensor([   0,    0,    0,    0,    0,    0, 6255, 8277])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> <end> sepsis' decode(Ytrain[i])='<end> <end> <end> <end> <end> <end> sepsis happens'\n",
      "tensor([   0,    0,    0,    0,    0,    0, 6255, 8277]) tensor([    0,     0,     0,     0,     0,  6255,  8277, 10733])\n",
      "decode(Xtrain[i])='<end> <end> <end> <end> <end> <end> sepsis happens' decode(Ytrain[i])='<end> <end> <end> <end> <end> sepsis happens when'\n"
     ]
    }
   ],
   "source": [
    "# shape the data for training\n",
    "def chunk(s):\n",
    "    s = torch.cat((torch.zeros(block_size, dtype=torch.long), s))\n",
    "    for i in range(0, len(s) - block_size):\n",
    "        yield s[i: i + block_size], s[i + 1: i + block_size + 1]\n",
    "\n",
    "chunked = [c for s in sentences for c in chunk(encode(s))]\n",
    "Xtrain = [c[0] for c in chunked]\n",
    "Ytrain = [c[1] for c in chunked]\n",
    "\n",
    "for i in range(3):\n",
    "    print(Xtrain[i], Ytrain[i])\n",
    "    print(f'{decode(Xtrain[i])=} {decode(Ytrain[i])=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "84bd70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,     0,     0,     0,  3835,  8826],\n",
      "        [ 5054,  6302,  3088, 10753,  6977,  2188,    11,  4845]])\n",
      "tensor([[    0,     0,     0,     0,     0,  3835,  8826,   928],\n",
      "        [ 6302,  3088, 10753,  6977,  2188,    11,  4845,  5054]])\n",
      "<end> <end> <end> <end> <end> <end> variations involve -> <end> <end> <end> <end> <end> variations involve replacing\n",
      "and can see long distances shapes shadows color -> can see long distances shapes shadows color and\n"
     ]
    }
   ],
   "source": [
    "def get_batch():\n",
    "    # TODO: swap between train and val\n",
    "    ix = torch.randint(len(Xtrain), (batch_size,))\n",
    "    x = torch.stack([Xtrain[i] for i in ix])\n",
    "    y = torch.stack([Ytrain[i] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb[:2])\n",
    "print(yb[:2])\n",
    "print(f'{decode(xb[0])} -> {decode(yb[0])}')\n",
    "print(f'{decode(xb[1])} -> {decode(yb[1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4f331",
   "metadata": {},
   "source": [
    "## Implem the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f02b855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 13479])\n",
      "9.364344596862793\n",
      "tensor([ 0.5058,  0.2564,  0.0894,  ..., -0.0993,  0.1320, -0.3849],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0xdeadbeef) # for reproducibility\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd), nn.ReLU(),\n",
    "            # nn.Linear(n_embd, n_embd), nn.BatchNorm1d(n_embd), nn.ReLU(),\n",
    "            # nn.Linear(n_embd, n_embd), nn.BatchNorm1d(num_features=n_embd), nn.ReLU(),\n",
    "        )\n",
    "        # self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # print(f'{idx.shape=} {targets.shape=}')\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        x = self.layers(tok_emb)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # juggle with tensor shapes to match pytorch's cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = LM()\n",
    "# m = model.to(device)\n",
    "m = model\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64677e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c98f7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.3797"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 500: train loss 3.8205\n",
      "step 1000: train loss 4.1586\n",
      "step 1500: train loss 4.1597\n",
      "step 2000: train loss 4.0523\n",
      "step 2500: train loss 3.4045\n",
      "step 3000: train loss 3.4417\n",
      "step 3500: train loss 3.5563\n",
      "step 4000: train loss 3.7179\n",
      "step 4500: train loss 3.3655\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for iter in range(max_iters):\n",
    "    # if iter % eval_interval == 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        print(f'step {iter}: train loss {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2aa748d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<end> <end> <end> <end> <end> <end> <end> heat or nine emperors <end> <end> steel and off from the out thinly flowers and sizes of their spending a milky are succulent are former bays that are omnivorous themselves like more difficult when they have important role at institutions are constant muscles of families with a form of stroke days have a parameter or near their molecules join in cold jobs the making enter protective trees use the harness react actually different substances <end> <end> <end> <end> <end> environmentalists dead holes to british different music music <end> <end> <end> <end> children have fewer services shows and an organism microbes that they belong to make tools on earth if different countries about what caused characters for an lead compounds are sloped out sizes and make a solvent extraction relating facts for the begin to use page are noise to hear when a style doing are made of papier in the ropes are larger than open to find based on the universe are a staircase on the coast and material is a collar on painting it is to cave appear with other parts of running vote <end> <end> <end> <end> <end> <end> <end> <end> <end> some lichens are indented under <end> <end> <end> <end> <end> <end> <end> <end> many religions have one about using and more frequent all ways for one special daily consider different items code is the short to be found in a different body or other things therapy is used for the ammunition and thus have two rules can lead telluride is very slight halophiles carry batons are made up and good or near the zygote with group spots to blue parietal <end> most of azerbaijan things both from going to be caused by putting the distance or threats which is landlocked\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "def decode(t):\n",
    "    return ' '.join([itos[i] for i in t])\n",
    "\n",
    "device='cpu'\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
