{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0270702",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de17ed3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caea3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dce284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "threshold = 10\n",
    "context_size = 5 # how many words on each side\n",
    "n_embd = 96\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "max_iters = 50000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "butchering_length = context_size # how many words to keep around my special topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f806ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text)=100000000\n",
      "len(text.split())=17005207\n",
      "alphabet = \" abcdefghijklmnopqrstuvwxyz\"\n",
      "vocab[:10]=['neglects', 'skunk', 'smallpipe', 'neuber', 'diagrammar', 'oor', 'uncapher', 'ziers', 'gintime', 'aliyah']\n",
      "len(vocab)=253854\n"
     ]
    }
   ],
   "source": [
    "# read another dataset\n",
    "# http://mattmahoney.net/dc/textdata.html\n",
    "\n",
    "with open('text8', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'{len(text)=}')\n",
    "print(f'{len(text.split())=}')\n",
    "print(f'alphabet = \"{\"\".join(sorted(set(text)))}\"')\n",
    "\n",
    "vocab = list(set(text.split()))\n",
    "print(f'{vocab[:10]=}')\n",
    "print(f'{len(vocab)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6abcdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted(vocab)[:100]=['a', 'aa', 'aaa', 'aaaa', 'aaaaaacceglllnorst', 'aaaaaaccegllnorrst', 'aaaaaah', 'aaaaaalmrsstt', 'aaaaaannrstyy', 'aaaaabbcdrr', 'aaaaargh', 'aaaargh', 'aaaassembly', 'aaab', 'aaabbbccc', 'aaahh', 'aaai', 'aaake', 'aaan', 'aaargh', 'aaas', 'aaate', 'aab', 'aababb', 'aabach', 'aabba', 'aabbcc', 'aabbirem', 'aabebwuvev', 'aabehlpt', 'aabmup', 'aabre', 'aabybro', 'aac', 'aaca', 'aacca', 'aaccording', 'aachen', 'aachener', 'aachtopf', 'aaci', 'aacis', 'aacisuan', 'aacplus', 'aacr', 'aacs', 'aacvd', 'aad', 'aadgad', 'aadl', 'aadlik', 'aadnani', 'aadvantage', 'aadyam', 'aaemu', 'aaf', 'aafc', 'aafjes', 'aafk', 'aafp', 'aag', 'aagaard', 'aagama', 'aagard', 'aage', 'aagesen', 'aagsin', 'aah', 'aahaaram', 'aahc', 'aahe', 'aahl', 'aahz', 'aai', 'aaib', 'aaiieee', 'aaimmah', 'aairpass', 'aaiun', 'aaiyangar', 'aaj', 'aajker', 'aak', 'aakirkeby', 'aakjaer', 'aakkram', 'aal', 'aalberg', 'aalborg', 'aalborghus', 'aalborgt', 'aalcc', 'aale', 'aalen', 'aalens', 'aalesund', 'aalesunds', 'aaliyah', 'aals', 'aalst']\n",
      "cs.most_common(100)=[('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644), ('is', 183153), ('as', 131815), ('eight', 125285), ('for', 118445), ('s', 116710), ('five', 115789), ('three', 114775), ('was', 112807), ('by', 111831), ('that', 109510), ('four', 108182), ('six', 102145), ('seven', 99683), ('with', 95603), ('on', 91250), ('are', 76527), ('it', 73334), ('from', 72871), ('or', 68945), ('his', 62603), ('an', 61925), ('be', 61281), ('this', 58832), ('which', 54788), ('at', 54576), ('he', 53573), ('also', 44358), ('not', 44033), ('have', 39712), ('were', 39086), ('has', 37866), ('but', 35358), ('other', 32433), ('their', 31523), ('its', 29567), ('first', 28810), ('they', 28553), ('some', 28161), ('had', 28100), ('all', 26229), ('more', 26223), ('most', 25563), ('can', 25519), ('been', 25383), ('such', 24413), ('many', 24096), ('who', 23997), ('new', 23770), ('used', 22737), ('there', 22707), ('after', 21125), ('when', 20623), ('into', 20484), ('american', 20477), ('time', 20412), ('these', 19864), ('only', 19463), ('see', 19206), ('may', 19115), ('than', 18807), ('world', 17949), ('i', 17581), ('b', 17516), ('would', 17377), ('d', 17236), ('no', 16155), ('however', 15861), ('between', 15737), ('about', 15574), ('over', 15122), ('years', 14935), ('states', 14916), ('people', 14696), ('war', 14629), ('during', 14578), ('united', 14494), ('known', 14437), ('if', 14420), ('called', 14151), ('use', 14011), ('th', 13380), ('system', 13296), ('often', 12987), ('state', 12904), ('so', 12722), ('history', 12623), ('will', 12560), ('up', 12445), ('while', 12363), ('where', 12347)]\n",
      "cs[\"aaaaaacceglllnorst\"]=1\n",
      "[(1, 118519), (2, 35297), (3, 17742), (4, 11006), (5, 7649), (6, 5529), (7, 4391), (8, 3626), (9, 2961), (10, 2523)]\n",
      "cs[\"king\"]=7456 cs[\"queen\"]=1940\n"
     ]
    }
   ],
   "source": [
    "# how crappy is my dataset ? :(\n",
    "from collections import Counter\n",
    "\n",
    "# looking at the sorted vocab give me very low confidence in the dataset quality\n",
    "print(f'{sorted(vocab)[:100]=}')\n",
    "cs = Counter(text.split())\n",
    "print(f'{cs.most_common(100)=}')\n",
    "print(f'{cs[\"aaaaaacceglllnorst\"]=}')\n",
    "\n",
    "# lots of words are only mentioned once\n",
    "ccs = Counter(cs.values())\n",
    "print(ccs.most_common(10))\n",
    "\n",
    "# on the plus side 'queen' and 'king' seem well represented\n",
    "print(f'{cs[\"king\"]=} {cs[\"queen\"]=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39592be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(butchered_vocab)=47134\n",
      "len(butchered_text)=16561031\n"
     ]
    }
   ],
   "source": [
    "# let's butcher the dataset ¯\\_(ツ)_/¯\n",
    "# remove all the words that are only mentioned bellow a threshold\n",
    "butchered_vocab = [w for w, c in cs.items() if c >= threshold]\n",
    "butchered_vocab_s = set(butchered_vocab)\n",
    "butchered_text = [w for w in text.split() if w in butchered_vocab_s]\n",
    "\n",
    "print(f'{len(butchered_vocab)=}')\n",
    "print(f'{len(butchered_text)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84c3f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w='banana': ct=111\n",
      "w='chicken': ct=241\n",
      "w='pink': ct=249\n",
      "w='purple': ct=267\n",
      "w='sheep': ct=305\n",
      "w='elephant': ct=314\n",
      "w='cow': ct=332\n",
      "w='princess': ct=504\n",
      "w='bird': ct=596\n",
      "w='fruit': ct=601\n",
      "w='orange': ct=607\n",
      "w='orange': ct=607\n",
      "w='cat': ct=692\n",
      "w='yellow': ct=733\n",
      "w='lady': ct=812\n",
      "w='dog': ct=958\n",
      "w='fish': ct=1174\n",
      "w='horse': ct=1180\n",
      "w='apple': ct=1465\n",
      "w='sir': ct=1488\n",
      "w='duke': ct=1611\n",
      "w='prince': ct=1687\n",
      "w='woman': ct=1753\n",
      "w='queen': ct=1940\n",
      "w='green': ct=2074\n",
      "w='blue': ct=2223\n",
      "w='lord': ct=2231\n",
      "w='male': ct=2615\n",
      "w='female': ct=2797\n",
      "w='red': ct=3755\n",
      "w='man': ct=5778\n",
      "w='king': ct=7456\n",
      "[['greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy'], ['they', 'held', 'that', 'the', 'good', 'man', 'will', 'be', 'guided', 'at', 'every'], ['that', 'warren', 'was', 'the', 'first', 'man', 'to', 'expound', 'and', 'formulate', 'the']]\n",
      "len(butchered_text)=16561031 len(butchered_text2)=534039\n",
      "len(butchered_vocab)=47134 len(butchered_vocab2)=25518\n"
     ]
    }
   ],
   "source": [
    "# still doesn't work, second round of butchering ¯\\_(ツ)_/¯\n",
    "# new idea, keep the context around the words I care about learning, and trash the rest of the dataset\n",
    "\n",
    "words_i_want_to_learn = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink',\n",
    "    'king', 'queen', 'prince', 'princess', 'duke', 'lord', 'lady', 'sir', 'man', 'woman', 'male', 'female',\n",
    "    'fruit', 'apple', 'orange', 'banana',\n",
    "    'dog', 'cat', 'horse', 'cow', 'chicken', 'bird', 'fish', 'sheep', 'elephant'\n",
    "]\n",
    "\n",
    "# words I wanted but they are kinda rare in my dataset:\n",
    "# ['lemon', 'lime', 'watermelon', 'clementine', 'tangerine', 'madam', 'mango', 'strawberry', 'pear', 'coconut', 'kiwi', 'duchess', 'grape',]\n",
    "\n",
    "for ct, w in sorted([(cs[w], w) for w in words_i_want_to_learn]):\n",
    "    print(f'{w=}: {ct=}')\n",
    "\n",
    "topics_whitelist = set(words_i_want_to_learn)\n",
    "butchered_sentences2 = []\n",
    "for i in range(butchering_length, len(butchered_text) - butchering_length):\n",
    "    w = butchered_text[i]\n",
    "    if w in topics_whitelist:\n",
    "        butchered_sentences2.append(butchered_text[i - butchering_length: i + butchering_length + 1])\n",
    "\n",
    "butchered_text2 = [w for s in butchered_sentences2 for w in s]\n",
    "butchered_vocab2 = sorted(set(butchered_text2))\n",
    "\n",
    "print(butchered_sentences2[:3])\n",
    "print(f'{len(butchered_text)=} {len(butchered_text2)=}')\n",
    "print(f'{len(butchered_vocab)=} {len(butchered_vocab2)=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f3590f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode(xs.split())=tensor([12895, 18713,   975,  9438])\n",
      "decode(encode(xs.split()))='kings queens and fruits'\n",
      "encode(xs.split())=tensor([24989,  4145,  2648, 11175, 16512,  3187,  5497])\n",
      "decode(encode(xs.split()))='white chicken black horse or brown cow'\n"
     ]
    }
   ],
   "source": [
    "# encode/decode helpers\n",
    "vocab_size = len(butchered_vocab2)\n",
    "stoi = {w: i for i, w in enumerate(butchered_vocab2)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "def encode(ws):\n",
    "    return torch.tensor([stoi[w] for w in ws], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    t = t.tolist() if isinstance(t, torch.Tensor) else t\n",
    "    t = [t] if isinstance(t, int) else t\n",
    "    return ' '.join([itos[i] for i in t])\n",
    "\n",
    "# for xs in ['i for one welcome our new robot overlords', 'the chicken cross the road']:\n",
    "for xs in ['kings queens and fruits', 'white chicken black horse or brown cow']:\n",
    "    print(f'{encode(xs.split())=}')\n",
    "    print(f'{decode(encode(xs.split()))=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "040519f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([534029, 1]) torch.Size([534029, 10])\n",
      "tensor([12889]) tensor([10181, 25152,  1355, 20073,  4151,   952,  1506,     0, 17792, 17428])\n",
      "decode(X[i])='king' decode(Y[i])='greek without archons ruler chief anarchism as a political philosophy'\n",
      "tensor([952]) tensor([25152,  1355, 20073,  4151, 12889,  1506,     0, 17792, 17428, 23097])\n",
      "decode(X[i])='anarchism' decode(Y[i])='without archons ruler chief king as a political philosophy they'\n",
      "tensor([1506]) tensor([ 1355, 20073,  4151, 12889,   952,     0, 17792, 17428, 23097, 10757])\n",
      "decode(X[i])='as' decode(Y[i])='archons ruler chief king anarchism a political philosophy they held'\n"
     ]
    }
   ],
   "source": [
    "# shape the data for training\n",
    "# using the skip-gram method\n",
    "def chunk(ws):\n",
    "    x, y = [], []\n",
    "    # miss a few words at the beginning and end of the text, w/e\n",
    "    for i in range(context_size, len(ws) - context_size):\n",
    "        x.append(ws[i])\n",
    "        # TODO: here a possible optimization would be to probabilistically discard some of the most common words\n",
    "        # the paper suggest proba to keep the word as:\n",
    "        # $P(w_i) = ({\\sqrt {z(w_i) \\over 0.001} + 1}) . {0.001 \\over z(w_i)}$\n",
    "        # z(w_i) being the frequency of the word in the corpus\n",
    "        y.append(torch.cat((ws[i - context_size: i], ws[i + 1: i + 1 + context_size])))\n",
    "    return torch.tensor(x).view(-1, 1), torch.stack(y)\n",
    "\n",
    "X, Y = chunk(encode(butchered_text2))\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    print(X[i], Y[i])\n",
    "    print(f'{decode(X[i])=} {decode(Y[i])=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19eae3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5851],\n",
      "        [11516]], device='cuda:0')\n",
      "tensor([[ 7245, 16341, 10469, 20495, 21939,  8981,  7522,  9276,  9302, 16384],\n",
      "        [16341,  8130, 11640,  1321,  7513, 13109, 16341, 12577, 13109, 16341]],\n",
      "       device='cuda:0')\n",
      "d -> duke of hamilton scottish statesman five eight four fran ois\n",
      "image -> of et in arcadia ego lady of jpg lady of\n"
     ]
    }
   ],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(len(X), (batch_size,))\n",
    "    x, y = X[ix], Y[ix]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb[:2])\n",
    "print(yb[:2])\n",
    "print(f'{decode(xb[0])} -> {decode(yb[0])}')\n",
    "print(f'{decode(xb[1])} -> {decode(yb[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd8dba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        x, y = get_batch()\n",
    "        logits, loss = model(x, y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a7774",
   "metadata": {},
   "source": [
    "## Skip-gram model\n",
    "given a word guess the (#context_size) words surrounding it.\n",
    "e.g. \"I for one welcome our robot overlords\"\n",
    "\n",
    "welcome -> for, one, our, robot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f67f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 25518])\n",
      "0.7337511777877808\n",
      "tensor([-1.0406, -0.2380, -0.2595,  ...,  0.0143, -0.7241, -0.3411],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0xdeadbeef) # for reproducibility\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # print(f'{idx.shape=} {targets.shape=}')\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        # x = self.layers(tok_emb)\n",
    "        logits = self.head(tok_emb) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # juggle with tensor shapes to match pytorch's cross_entropy\n",
    "            # print(f'before: {logits.shape=} {targets.shape=}')\n",
    "            # B, T, C = logits.shape\n",
    "            # logits = logits.view(B * T, C)\n",
    "            # targets = targets.view(B * T)\n",
    "            # print(f'after:  {logits.shape=} {targets.shape=}')\n",
    "            # loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # TODO: I give up on the clean solution for now, using an expected logits instead of ids\n",
    "            # make expected a one-hot encoding\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets_logits = torch.zeros_like(logits)\n",
    "            rows = torch.arange(targets.shape[0]).view(-1, 1)\n",
    "\n",
    "            # print(f'{targets_logits.shape=} {rows.shape=} {targets.shape=}')\n",
    "            targets_logits[rows, targets] = 1\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets_logits)\n",
    "        return logits, loss\n",
    "    \n",
    "    # def generate(self, idx, max_new_tokens):\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # crop the context to the last block_size tokens\n",
    "    #         idx_cond = idx[:, -block_size:]\n",
    "    #         logits, loss = self(idx_cond)\n",
    "    #         logits = logits[:, -1, :]\n",
    "    #         probs = F.softmax(logits, dim=-1)\n",
    "    #         idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    #         idx = torch.cat((idx, idx_next), dim=1)\n",
    "    #     return idx\n",
    "    \n",
    "model = LM()\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aea9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear my head\n",
    "mini_batch_size = 3\n",
    "mini_embedding_size = 8\n",
    "mini_surroundings = 4\n",
    "\n",
    "logits = torch.ones((mini_batch_size, mini_embedding_size))\n",
    "expected = torch.tensor([(0, 1, 3, 7), (0, 2, 4, 6), (4, 5, 6, 7)], dtype=torch.long)\n",
    "# make expected a one-hot encoding\n",
    "expected_logits = torch.zeros_like(logits)\n",
    "rows = torch.arange(mini_batch_size).view(-1, 1)\n",
    "expected_logits[rows, expected] = 1\n",
    "\n",
    "expected_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6aece823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf3bab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.7340\n",
      "step 500: train loss 0.5749\n",
      "step 1000: train loss 0.4518\n",
      "step 1500: train loss 0.3622\n",
      "step 2000: train loss 0.3108\n",
      "step 2500: train loss 0.2731\n",
      "step 3000: train loss 0.2405\n",
      "step 3500: train loss 0.2190\n",
      "step 4000: train loss 0.1982\n",
      "step 4500: train loss 0.1794\n",
      "step 5000: train loss 0.1674\n",
      "step 5500: train loss 0.1562\n",
      "step 6000: train loss 0.1406\n",
      "step 6500: train loss 0.1369\n",
      "step 7000: train loss 0.1225\n",
      "step 7500: train loss 0.1144\n",
      "step 8000: train loss 0.1071\n",
      "step 8500: train loss 0.0999\n",
      "step 9000: train loss 0.0932\n",
      "step 9500: train loss 0.0870\n",
      "step 10000: train loss 0.0808\n",
      "step 10500: train loss 0.0747\n",
      "step 11000: train loss 0.0721\n",
      "step 11500: train loss 0.0663\n",
      "step 12000: train loss 0.0628\n",
      "step 12500: train loss 0.0594\n",
      "step 13000: train loss 0.0553\n",
      "step 13500: train loss 0.0522\n",
      "step 14000: train loss 0.0486\n",
      "step 14500: train loss 0.0479\n",
      "step 15000: train loss 0.0435\n",
      "step 15500: train loss 0.0415\n",
      "step 16000: train loss 0.0378\n",
      "step 16500: train loss 0.0360\n",
      "step 17000: train loss 0.0342\n",
      "step 17500: train loss 0.0326\n",
      "step 18000: train loss 0.0286\n",
      "step 18500: train loss 0.0281\n",
      "step 19000: train loss 0.0261\n",
      "step 19500: train loss 0.0241\n",
      "step 20000: train loss 0.0247\n",
      "step 20500: train loss 0.0233\n",
      "step 21000: train loss 0.0216\n",
      "step 21500: train loss 0.0198\n",
      "step 22000: train loss 0.0188\n",
      "step 22500: train loss 0.0176\n",
      "step 23000: train loss 0.0171\n",
      "step 23500: train loss 0.0155\n",
      "step 24000: train loss 0.0150\n",
      "step 24500: train loss 0.0148\n",
      "step 25000: train loss 0.0130\n",
      "step 25500: train loss 0.0135\n",
      "step 26000: train loss 0.0125\n",
      "step 26500: train loss 0.0126\n",
      "step 27000: train loss 0.0109\n",
      "step 27500: train loss 0.0104\n",
      "step 28000: train loss 0.0101\n",
      "step 28500: train loss 0.0102\n",
      "step 29000: train loss 0.0096\n",
      "step 29500: train loss 0.0088\n",
      "step 30000: train loss 0.0085\n",
      "step 30500: train loss 0.0081\n",
      "step 31000: train loss 0.0079\n",
      "step 31500: train loss 0.0075\n",
      "step 32000: train loss 0.0072\n",
      "step 32500: train loss 0.0066\n",
      "step 33000: train loss 0.0066\n",
      "step 33500: train loss 0.0064\n",
      "step 34000: train loss 0.0059\n",
      "step 34500: train loss 0.0059\n",
      "step 35000: train loss 0.0057\n",
      "step 35500: train loss 0.0055\n",
      "step 36000: train loss 0.0053\n",
      "step 36500: train loss 0.0051\n",
      "step 37000: train loss 0.0049\n",
      "step 37500: train loss 0.0047\n",
      "step 38000: train loss 0.0047\n",
      "step 38500: train loss 0.0046\n",
      "step 39000: train loss 0.0043\n",
      "step 39500: train loss 0.0042\n",
      "step 40000: train loss 0.0041\n",
      "step 40500: train loss 0.0041\n",
      "step 41000: train loss 0.0040\n",
      "step 41500: train loss 0.0039\n",
      "step 42000: train loss 0.0037\n",
      "step 42500: train loss 0.0036\n",
      "step 43000: train loss 0.0035\n",
      "step 43500: train loss 0.0035\n",
      "step 44000: train loss 0.0034\n",
      "step 44500: train loss 0.0034\n",
      "step 45000: train loss 0.0033\n",
      "step 45500: train loss 0.0033\n",
      "step 46000: train loss 0.0031\n",
      "step 46500: train loss 0.0032\n",
      "step 47000: train loss 0.0031\n",
      "step 47500: train loss 0.0030\n",
      "step 48000: train loss 0.0029\n",
      "step 48500: train loss 0.0028\n",
      "step 49000: train loss 0.0029\n",
      "step 49500: train loss 0.0029\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        loss = estimate_loss()\n",
    "        print(f'step {iter}: train loss {loss:.4f}')\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bda7f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup to disk\n",
    "torch.save(model.state_dict(), 'skip-gram-colors.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bed7b",
   "metadata": {},
   "source": [
    "## can we use embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8bef9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def euclidean_dist(a, b):\n",
    "    return torch.sqrt(torch.sum((a - b) ** 2))\n",
    "\n",
    "# Cosine distance\n",
    "def cosine_dist(a, b):\n",
    "    # return 1 - (a @ b.T) / (torch.sqrt(torch.sum(a**2)) * torch.sqrt(torch.sum(b**2)))\n",
    "    return 1 - torch.nn.functional.cosine_similarity(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7383b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96])\n",
      "euclidean_dist(king, queen)=tensor(16.1675, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
      "cosine_dist(king, queen)=tensor(0.8709, device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word, m=m):\n",
    "    return m.token_embedding_table(torch.tensor(stoi[word], dtype=torch.long, device=device))\n",
    "\n",
    "king = get_embedding('king')\n",
    "queen = get_embedding('queen')\n",
    "\n",
    "print(king.shape)\n",
    "\n",
    "print(f'{euclidean_dist(king, queen)=}')\n",
    "print(f'{cosine_dist(king, queen)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "470703d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean_dist(king, soap)=tensor(15.9545, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
      "cosine_dist(king, soap)=tensor(1.0272, device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "soap = get_embedding('soap')\n",
    "\n",
    "print(f'{euclidean_dist(king, soap)=}')\n",
    "print(f'{cosine_dist(king, soap)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db43271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7027 king prince\n",
      "0.7160 king man\n",
      "0.7169 man woman\n",
      "0.7620 queen man\n",
      "0.7801 prince man\n",
      "0.7908 princess green\n",
      "0.8057 prince princess\n",
      "0.8133 woman lettuce\n",
      "0.8163 prince woman\n",
      "0.8267 queen princess\n",
      "0.8368 king woman\n",
      "0.8507 princess man\n",
      "0.8530 princess woman\n",
      "0.8533 queen prince\n",
      "0.8588 king lettuce\n",
      "0.8589 queen woman\n",
      "0.8621 woman green\n",
      "0.8641 king princess\n",
      "0.8680 prince green\n",
      "0.8709 king queen\n",
      "0.8719 queen green\n",
      "0.8720 queen lettuce\n",
      "0.8826 prince lettuce\n",
      "0.9015 man green\n",
      "0.9157 princess lettuce\n",
      "0.9616 man lettuce\n",
      "0.9851 king green\n",
      "1.0808 lettuce green\n"
     ]
    }
   ],
   "source": [
    "# cross relations\n",
    "\n",
    "import itertools\n",
    "words = ['king', 'queen', 'prince', 'princess', 'man', 'woman', 'lettuce', 'green']\n",
    "# words = ['flower', 'bee', 'dog', 'bone']\n",
    "# words = ['yolk', 'egg', 'chicken', 'computer']\n",
    "# words = ['sun', 'warm', 'snow', 'cold']\n",
    "pairs = []\n",
    "for a, b in itertools.combinations(words, 2):\n",
    "    emb_a = get_embedding(a)\n",
    "    emb_b = get_embedding(b)\n",
    "    pairs.append((cosine_dist(emb_a, emb_b), a, b))\n",
    "\n",
    "for d, a, b in sorted(pairs):\n",
    "    print(f'{d:.4f} {a} {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = list(butchered_vocab_s)\n",
    "lw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdf1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most related words\n",
    "# word = 'king'\n",
    "# word = 'flower'\n",
    "word = 'banana'\n",
    "emb_word = get_embedding(word)\n",
    "matches = []\n",
    "for w in lw[:]:\n",
    "    if cs[w] < 100:\n",
    "        continue\n",
    "    emb_w = get_embedding(w)\n",
    "    d = cosine_dist(emb_word, emb_w)\n",
    "    matches.append((d, w))\n",
    "\n",
    "for d, w in sorted(matches)[:10]:\n",
    "    print(f'{d:.4f} {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c6f9f3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'red green blue yellow orange purple pink king queen prince princess duke lord lady sir man woman male female fruit apple orange banana dog cat horse cow chicken bird fish sheep elephant'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words to choose from:\n",
    "' '.join(words_i_want_to_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "90f3c428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- trained ---\n",
      "0.0000 female\n",
      "0.7115 chicken\n",
      "0.7465 male\n",
      "0.7636 sir\n",
      "0.7689 queen\n",
      "0.7751 orange\n",
      "0.7751 orange\n",
      "0.7887 fruit\n",
      "0.7978 sheep\n",
      "0.8121 red\n",
      "--- untrained ---\n",
      "0.0000 female\n",
      "0.7638 man\n",
      "0.8519 blue\n",
      "0.8983 fish\n",
      "0.8984 green\n",
      "0.9107 fruit\n",
      "0.9157 horse\n",
      "0.9302 sheep\n",
      "0.9318 sir\n",
      "0.9402 orange\n"
     ]
    }
   ],
   "source": [
    "# compare trained and untrained model\n",
    "# <???> why is 'cow' so popular ?\n",
    "word = 'yellow'\n",
    "word = 'pink'\n",
    "word = 'prince'\n",
    "word = 'female'\n",
    "\n",
    "untrained_model = LM()\n",
    "um = untrained_model.to(device)\n",
    "\n",
    "for mm, label in [(m, 'trained'), (untrained_model, 'untrained')]:\n",
    "    print(f'--- {label} ---')\n",
    "    emb_word = get_embedding(word, m=mm)\n",
    "    matches = []\n",
    "    # for w in lw[:]:\n",
    "    for w in words_i_want_to_learn:\n",
    "        if cs[w] < 100:\n",
    "            continue\n",
    "        emb_w = get_embedding(w, m=mm)\n",
    "        d = cosine_dist(emb_word, emb_w)\n",
    "        matches.append((d, w))\n",
    "\n",
    "    for d, w in sorted(matches)[:10]:\n",
    "        print(f'{d:.4f} {w}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word arithmetic (not gonna work, but let me dream)\n",
    "emb_word = get_embedding('king') - get_embedding('man') + get_embedding('woman')\n",
    "\n",
    "matches = []\n",
    "for w in lw[:]:\n",
    "    if cs[w] < 100:\n",
    "        continue\n",
    "    emb_w = get_embedding(w)\n",
    "    d = cosine_dist(emb_word, emb_w)\n",
    "    matches.append((d, w))\n",
    "\n",
    "for d, w in sorted(matches)[:10]:\n",
    "    print(f'{d:.4f} {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs['banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems pretty random to me, cabbage is closer to queen than king is closer to queen :/\n",
    "cabbage = get_embedding('cabbage')\n",
    "print(f'{euclidean_dist(cabbage, queen)=}')\n",
    "print(f'{cosine_dist(cabbage, queen)=}')\n",
    "\n",
    "shadows = get_embedding('shadows')\n",
    "print(f'{euclidean_dist(shadows, queen)=}')\n",
    "print(f'{cosine_dist(shadows, queen)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296347bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all embeddings\n",
    "embds = torch.stack([get_embedding(w) for w in butchered_vocab_s])\n",
    "embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all pairwise distances\n",
    "def pairwise_euclidean_distance(embds):\n",
    "    xx = torch.sum(embds**2, dim=1)\n",
    "    xy = embds @ embds.T\n",
    "    x2 = xx.view(-1, 1)\n",
    "    return x2 - 2 * xy + xx\n",
    "\n",
    "pwed = pairwise_euclidean_distance(embds)\n",
    "print(pwed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the closest word for each other word in the vocab:\n",
    "#\n",
    "# This is totally non-sensical :(\n",
    "# I don't see any pattern in the results \n",
    "# hyp 1: the dataset is too sparse, only a few mention of each words\n",
    "# hyp 2: the model is too small, it's not able to learn anything\n",
    "# hyp 3: Unknown-unknown, I messed up something\n",
    "e = pwed\n",
    "mask = (torch.ones_like(e) * float('inf')).tril()\n",
    "vals, ind = torch.min((e + mask), dim=1)\n",
    "\n",
    "for i, j in enumerate(ind[:-1]):\n",
    "    print(f'{itos[i]} {itos[j.item()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65014c",
   "metadata": {},
   "source": [
    "## compare with gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f14229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "mx2 = Word2Vec(sentences=text.split(), vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b06a0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = set(text.split())\n",
    "'king' in ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81eec933",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'king' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\word2vec\\nano_word2vec_skip-gram.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/p/Desktop/_ML/word2vec/nano_word2vec_skip-gram.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mx\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m'\u001b[39;49m\u001b[39mking\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'king' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# WHY ARE YOU NOT DOING THE THING ?! >:@\n",
    "mx2.wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa59bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define your dataset as a list of sentences\n",
    "sentences = [\n",
    "    \"The king ruled the kingdom\",\n",
    "    \"The queen ruled the queendom\",\n",
    "    \"The man is strong\",\n",
    "    \"The woman is smart\",\n",
    "    \"The prince is young\",\n",
    "    \"The princess is graceful\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog barked loudly\",\n",
    "    \"The lion roared in the jungle\",\n",
    "    \"The tiger prowled through the forest\",\n",
    "    \"The red rose is beautiful\",\n",
    "    \"The sky is blue\",\n",
    "    \"The grass is green\",\n",
    "    \"The sun is shining yellow\",\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "# print(tokenized_sentences)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "mx = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# mx.wv['king']\n",
    "\n",
    "# Perform word arithmetic\n",
    "result = mx.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
